[
  {
    "context_before_main_text": [],
    "main_text": [
      {
        "sid": "s000001",
        "text": "Decision and in general.",
        "start": 560,
        "end": 2320,
        "speaker": "A"
      },
      {
        "sid": "s000002",
        "text": "Today we'll talk about three broad frameworks that I will ask you probably to apply on an exam or in some context.",
        "start": 2320,
        "end": 8960,
        "speaker": "A"
      },
      {
        "sid": "s000003",
        "text": "They are consequential frameworks where we're concerned more about the actual outcomes or consequences of a particular action over the course of making a decision.",
        "start": 9520,
        "end": 17200,
        "speaker": "A"
      },
      {
        "sid": "s000004",
        "text": "A duty based framework that is more concerned with an agent's intent rather than the actual consequences of what happens.",
        "start": 18160,
        "end": 24080,
        "speaker": "A"
      },
      {
        "sid": "s000005",
        "text": "And in the virtue based framework, which is more about the ethical makeup of an agent, in my mind, there's a pretty clear line between consequential and duty based.",
        "start": 24240,
        "end": 32680,
        "speaker": "A"
      },
      {
        "sid": "s000006",
        "text": "But duty and virtue kind of make it a harder thing to delineate.",
        "start": 32920,
        "end": 36880,
        "speaker": "A"
      },
      {
        "sid": "s000007",
        "text": "So we'll talk about sort of how to weave between those and give some examples.",
        "start": 36880,
        "end": 40600,
        "speaker": "A"
      },
      {
        "sid": "s000008",
        "text": "But let's start with a consequentialist framework.",
        "start": 41479,
        "end": 46520,
        "speaker": "A"
      },
      {
        "sid": "s000009",
        "text": "There are a variety of people who have looked at this in general, but John Stuart Mill and Jerry Bentham, who was the predecessor for him, are probably the bigger names in this area.",
        "start": 47640,
        "end": 55550,
        "speaker": "A"
      },
      {
        "sid": "s000010",
        "text": "You probably know it as utilitarianism.",
        "start": 56110,
        "end": 57950,
        "speaker": "A"
      },
      {
        "sid": "s000011",
        "text": "Utilitarianism is the idea that you've got.",
        "start": 58750,
        "end": 62590,
        "speaker": "A"
      },
      {
        "sid": "s000012",
        "text": "You're looking at the sum of.",
        "start": 65230,
        "end": 67310,
        "speaker": "A"
      },
      {
        "sid": "s000013",
        "text": "So if you're trying to look for something that maximizes its utility, that you're looking for an outcome that maximizes the general good as a whole, it can be tricky.",
        "start": 77450,
        "end": 86810,
        "speaker": "A"
      },
      {
        "sid": "s000014",
        "text": "You might say that better actions promote more happiness, and so more overall happiness is a better action.",
        "start": 87930,
        "end": 95290,
        "speaker": "A"
      },
      {
        "sid": "s000015",
        "text": "And you can consider that pain is the opposite of that happiness.",
        "start": 95690,
        "end": 101130,
        "speaker": "A"
      },
      {
        "sid": "s000016",
        "text": "So you kind of subtract and miscalcules that doing in your head of how much pain something causes versus how much pleasure or happiness it causes.",
        "start": 101130,
        "end": 108420,
        "speaker": "A"
      },
      {
        "sid": "s000017",
        "text": "But what was key to this, and this is particularly true for Bentham, was that you're supposed to treat every person equally.",
        "start": 109460,
        "end": 115940,
        "speaker": "A"
      },
      {
        "sid": "s000018",
        "text": "So if all individuals have equal work, and my wife decides that it's time to go to LACMA and for all of us to work and me and my kid hated, then her happiness doesn't make our unhappiness.",
        "start": 117460,
        "end": 130859,
        "speaker": "A"
      },
      {
        "sid": "s000019",
        "text": "One person's happiness is more important than the other.",
        "start": 132939,
        "end": 135099,
        "speaker": "A"
      },
      {
        "sid": "s000020",
        "text": "So that kind of decision.",
        "start": 135659,
        "end": 136619,
        "speaker": "A"
      },
      {
        "sid": "s000021",
        "text": "I should have quoted this before I get started, is that you shouldn't.",
        "start": 136619,
        "end": 140459,
        "speaker": "A"
      },
      {
        "sid": "s000022",
        "text": "You should be looking at your overall happiness, not that one person's happiness is really.",
        "start": 140459,
        "end": 144299,
        "speaker": "A"
      },
      {
        "sid": "s000023",
        "text": "Now for utilitarianism, in some ways this can provide some concrete notion of what is right.",
        "start": 147579,
        "end": 153979,
        "speaker": "A"
      },
      {
        "sid": "s000024",
        "text": "You look, look at the outcomes and you try and figure out is this going to help more people than it hurts.",
        "start": 153979,
        "end": 158950,
        "speaker": "A"
      },
      {
        "sid": "s000025",
        "text": "But the weight of something being positive or negative becomes tricky.",
        "start": 160070,
        "end": 164390,
        "speaker": "A"
      },
      {
        "sid": "s000026",
        "text": "How is it clear how much pleasure a particular idea is giving me versus how much pain is causing someone else.",
        "start": 164550,
        "end": 169790,
        "speaker": "A"
      },
      {
        "sid": "s000027",
        "text": "How do you quantify that?",
        "start": 169790,
        "end": 170950,
        "speaker": "A"
      },
      {
        "sid": "s000028",
        "text": "And defining the utility to an individual is also kind of tricky.",
        "start": 171910,
        "end": 175110,
        "speaker": "A"
      },
      {
        "sid": "s000029",
        "text": "Now, the common arguments against utilitarianism are that it's insensitive to minority rights.",
        "start": 175510,
        "end": 182080,
        "speaker": "A"
      },
      {
        "sid": "s000030",
        "text": "It may be that the larger group of people who are getting benefit because treating everyone equally all the way into the same outweighs any pain to another set of individuals.",
        "start": 182880,
        "end": 193040,
        "speaker": "A"
      },
      {
        "sid": "s000031",
        "text": "And Bentham actually was hit upon this when he who was at his time, so this is a long time ago, was very much against slavery, was someone who was being push back on utilitarianism, saying, well, look, if it's only a certain subset that are getting affected and it's painful to them, that would be outweighed by the pleasure to all.",
        "start": 193680,
        "end": 214470,
        "speaker": "A"
      },
      {
        "sid": "s000032",
        "text": "But his argument against that was that there was a cost to your own.",
        "start": 214710,
        "end": 220070,
        "speaker": "A"
      },
      {
        "sid": "s000033",
        "text": "You want to think of it as a soul or whatever in doing such a cruel thing to another individual that that would be creating this overall outcome.",
        "start": 221110,
        "end": 229110,
        "speaker": "A"
      },
      {
        "sid": "s000034",
        "text": "He worked around it a little bit to try to make it work.",
        "start": 229190,
        "end": 231130,
        "speaker": "A"
      },
      {
        "sid": "s000035",
        "text": "But this, in a basic utilitarian strategy is the problem that you can justify any action, regardless of how cruel or evil it might be at somebody, as long as you have sufficient good outcomes.",
        "start": 231440,
        "end": 244960,
        "speaker": "A"
      },
      {
        "sid": "s000036",
        "text": "And so there are all kinds of examples in history of things that may seem like good outcomes.",
        "start": 245120,
        "end": 249680,
        "speaker": "A"
      },
      {
        "sid": "s000037",
        "text": "But the question is, was it a good action that was taken?",
        "start": 249680,
        "end": 252000,
        "speaker": "A"
      },
      {
        "sid": "s000038",
        "text": "And it raises the question, are there any actions out there that are just inherently unethical, something that we just shouldn't do, regardless of whether it has an outcome that ends up being good enough?",
        "start": 253360,
        "end": 264089,
        "speaker": "A"
      },
      {
        "sid": "s000039",
        "text": "Does that make sense to everybody?",
        "start": 265209,
        "end": 266489,
        "speaker": "A"
      },
      {
        "sid": "s000040",
        "text": "Any questions about utilitarianism?",
        "start": 268329,
        "end": 270169,
        "speaker": "A"
      },
      {
        "sid": "s000041",
        "text": "Again, there's plenty of books and articles and lots to read on this, but that's the basic idea.",
        "start": 270169,
        "end": 277209,
        "speaker": "A"
      },
      {
        "sid": "s000042",
        "text": "Another approach, another ethical framework, is the ontological thinking.",
        "start": 283860,
        "end": 288580,
        "speaker": "A"
      },
      {
        "sid": "s000043",
        "text": "The idea that you have a duty or an obligation, some things that you just do, a moral obligation that's supposed to govern your actions regardless of the outcome?",
        "start": 289300,
        "end": 300260,
        "speaker": "A"
      },
      {
        "sid": "s000044",
        "text": "So here it kind of flips utilitarianism.",
        "start": 300820,
        "end": 302820,
        "speaker": "A"
      },
      {
        "sid": "s000045",
        "text": "Instead of looking at was the outcome good or not, it's what was your intent?",
        "start": 302820,
        "end": 307220,
        "speaker": "A"
      },
      {
        "sid": "s000046",
        "text": "What was the outcome action you perform trying to qualify that before you see the actual outcome?",
        "start": 307220,
        "end": 311960,
        "speaker": "A"
      },
      {
        "sid": "s000047",
        "text": "The trick here is if you're going to say that certain things are good and certain things are bad, it's just a good thing to do, or it's a bad thing to do, and you're not going to try to quantify the outcome, but you're going to try to judge the action.",
        "start": 314520,
        "end": 326200,
        "speaker": "A"
      },
      {
        "sid": "s000048",
        "text": "How do you make a determination of what's good and what's bad?",
        "start": 326600,
        "end": 329800,
        "speaker": "A"
      },
      {
        "sid": "s000049",
        "text": "So there are all different ways that we as a society try and figure out what's right and wrong.",
        "start": 331080,
        "end": 336290,
        "speaker": "A"
      },
      {
        "sid": "s000050",
        "text": "There's an area of deontological thinking called contractarianism.",
        "start": 336930,
        "end": 342050,
        "speaker": "A"
      },
      {
        "sid": "s000051",
        "text": "And the idea here might be that it might make more sense for us to cooperate with one another and do things that benefit each other than try to go against one another.",
        "start": 344210,
        "end": 352930,
        "speaker": "A"
      },
      {
        "sid": "s000052",
        "text": "It's almost like the golden rule a little bit, but not exactly, but that we feel like we have a social contract and.",
        "start": 353170,
        "end": 358870,
        "speaker": "A"
      },
      {
        "sid": "s000053",
        "text": "And maybe we build political structures and systems that would allow us to raise everyone up by having what's good for the society be our driving force.",
        "start": 358940,
        "end": 370300,
        "speaker": "A"
      },
      {
        "sid": "s000054",
        "text": "That's.",
        "start": 371420,
        "end": 371900,
        "speaker": "A"
      },
      {
        "sid": "s000055",
        "text": "We can debate how well that works.",
        "start": 371900,
        "end": 373340,
        "speaker": "A"
      },
      {
        "sid": "s000056",
        "text": "Another approach is divine.",
        "start": 374140,
        "end": 375660,
        "speaker": "A"
      },
      {
        "sid": "s000057",
        "text": "Some people have the commandments that come from a higher power.",
        "start": 375820,
        "end": 379420,
        "speaker": "A"
      },
      {
        "sid": "s000058",
        "text": "A divine figure makes a decision.",
        "start": 380380,
        "end": 382060,
        "speaker": "A"
      },
      {
        "sid": "s000059",
        "text": "Socrates.",
        "start": 382700,
        "end": 383500,
        "speaker": "A"
      },
      {
        "sid": "s000060",
        "text": "Sorry, not Socrates.",
        "start": 383660,
        "end": 384400,
        "speaker": "A"
      },
      {
        "sid": "s000061",
        "text": "This is the one who raised the question.",
        "start": 384550,
        "end": 386670,
        "speaker": "A"
      },
      {
        "sid": "s000062",
        "text": "But Plato actually wrote about it.",
        "start": 386670,
        "end": 388550,
        "speaker": "A"
      },
      {
        "sid": "s000063",
        "text": "He wrote about Socrates looking at this dilemma called the.",
        "start": 388550,
        "end": 391990,
        "speaker": "A"
      },
      {
        "sid": "s000064",
        "text": "I can't pronounce it but euthyphro dilemma, which was basically, if a deity were to tell you that something is good, that you should be doing that something, is the something itself inherently good?",
        "start": 391990,
        "end": 403590,
        "speaker": "A"
      },
      {
        "sid": "s000065",
        "text": "Or is it because the deity told you that it's good?",
        "start": 403990,
        "end": 407510,
        "speaker": "A"
      },
      {
        "sid": "s000066",
        "text": "And in his mind, the dilemma between these two was if it was already inherently good, then we should figure out why it was good instead of just knowing that the ED told you to do it.",
        "start": 407830,
        "end": 418010,
        "speaker": "A"
      },
      {
        "sid": "s000067",
        "text": "So why go through that level of indirection?",
        "start": 418490,
        "end": 420290,
        "speaker": "A"
      },
      {
        "sid": "s000068",
        "text": "Let's just figure out why it's good inherently.",
        "start": 420290,
        "end": 422330,
        "speaker": "A"
      },
      {
        "sid": "s000069",
        "text": "Or if it was because someone or something decided it was good.",
        "start": 422970,
        "end": 427050,
        "speaker": "A"
      },
      {
        "sid": "s000070",
        "text": "That seems a little arbitrary.",
        "start": 427530,
        "end": 428930,
        "speaker": "A"
      },
      {
        "sid": "s000071",
        "text": "And it seems like morality is just arbitrary in the mind of one individual.",
        "start": 428930,
        "end": 433210,
        "speaker": "A"
      },
      {
        "sid": "s000072",
        "text": "So there's almost a question of it's kind of offloads it still as undecided.",
        "start": 434170,
        "end": 439380,
        "speaker": "A"
      },
      {
        "sid": "s000073",
        "text": "Another source was human reason.",
        "start": 440980,
        "end": 442580,
        "speaker": "A"
      },
      {
        "sid": "s000074",
        "text": "That we follow kind of a natural law as human beings, that there's something guiding us in principle.",
        "start": 443380,
        "end": 449380,
        "speaker": "A"
      },
      {
        "sid": "s000075",
        "text": "So these are all different approaches, but the one that we're going to cover in this class is manual.",
        "start": 449860,
        "end": 455940,
        "speaker": "A"
      },
      {
        "sid": "s000076",
        "text": "So his approach was we've got two imperatives that exist in nature.",
        "start": 456500,
        "end": 461320,
        "speaker": "A"
      },
      {
        "sid": "s000077",
        "text": "There's a hypothetical imperative, which is something that is good to do conditioned on something else.",
        "start": 462350,
        "end": 468030,
        "speaker": "A"
      },
      {
        "sid": "s000078",
        "text": "Maybe there's a decision that I make only in certain circumstances, but not always.",
        "start": 469470,
        "end": 473470,
        "speaker": "A"
      },
      {
        "sid": "s000079",
        "text": "That's tricky because you have to be able to know when the circumstance arises and what justifies that action.",
        "start": 474590,
        "end": 479630,
        "speaker": "A"
      },
      {
        "sid": "s000080",
        "text": "But in his mind, a stronger thing was a categorical imperative, an unconditional commit.",
        "start": 480510,
        "end": 485830,
        "speaker": "A"
      },
      {
        "sid": "s000081",
        "text": "Something that you should always do no matter what.",
        "start": 485830,
        "end": 487920,
        "speaker": "A"
      },
      {
        "sid": "s000082",
        "text": "So the categorical imperative came from two basic principles.",
        "start": 491360,
        "end": 495920,
        "speaker": "A"
      },
      {
        "sid": "s000083",
        "text": "You will have a universality to it.",
        "start": 496560,
        "end": 498600,
        "speaker": "A"
      },
      {
        "sid": "s000084",
        "text": "You want something that you can always act on regardless of the circumstances.",
        "start": 498600,
        "end": 502080,
        "speaker": "A"
      },
      {
        "sid": "s000085",
        "text": "There should not be a hypothetical that would change the categorical imperative.",
        "start": 502080,
        "end": 505600,
        "speaker": "A"
      },
      {
        "sid": "s000086",
        "text": "You want something that you would always pursue, something that is always the right choice, and you want it to become what's called the universal law.",
        "start": 505920,
        "end": 511840,
        "speaker": "A"
      },
      {
        "sid": "s000087",
        "text": "According to his other objective was humanity.",
        "start": 511840,
        "end": 516010,
        "speaker": "A"
      },
      {
        "sid": "s000088",
        "text": "He didn't lose sight of the idea that this should be something that benefits people and that people should not be simply a means to an end, but it should be something that you treat the humanity and other people as the end itself.",
        "start": 516970,
        "end": 529050,
        "speaker": "A"
      },
      {
        "sid": "s000089",
        "text": "Kanti and ethics are great if you can identify what those particular categorical imperatives are.",
        "start": 533530,
        "end": 539320,
        "speaker": "A"
      },
      {
        "sid": "s000090",
        "text": "If you can just know I should never lie, great.",
        "start": 539560,
        "end": 543320,
        "speaker": "A"
      },
      {
        "sid": "s000091",
        "text": "It's easy to follow.",
        "start": 543400,
        "end": 544440,
        "speaker": "A"
      },
      {
        "sid": "s000092",
        "text": "I have a clear something that makes sense, but it gets a little bit tricky.",
        "start": 544680,
        "end": 549960,
        "speaker": "A"
      },
      {
        "sid": "s000093",
        "text": "So the joke on the upper right is actually based on something that Kant was challenged on.",
        "start": 550600,
        "end": 556920,
        "speaker": "A"
      },
      {
        "sid": "s000094",
        "text": "They said to him, what if there's a crazed murderer wants to kill your friend and you know where your friend is, and the murderer asks you, should you lie and say to your friend, he said, no.",
        "start": 557480,
        "end": 568330,
        "speaker": "A"
      },
      {
        "sid": "s000095",
        "text": "Even then I won't lie because maybe my friend has moved and me telling a lie might actually hurt them because they moved to where my friend was.",
        "start": 568570,
        "end": 576330,
        "speaker": "A"
      },
      {
        "sid": "s000096",
        "text": "In his mind, doing what was right as a universal thing shouldn't be influenced by the outcome.",
        "start": 577210,
        "end": 583530,
        "speaker": "A"
      },
      {
        "sid": "s000097",
        "text": "It should always be that duty, not a great friend.",
        "start": 583530,
        "end": 585770,
        "speaker": "A"
      },
      {
        "sid": "s000098",
        "text": "Okay, in terms of weaknesses, extremely rigid.",
        "start": 585850,
        "end": 591220,
        "speaker": "A"
      },
      {
        "sid": "s000099",
        "text": "How many things can you say are universal that are imperatives that you would always want to do?",
        "start": 591540,
        "end": 596420,
        "speaker": "A"
      },
      {
        "sid": "s000100",
        "text": "What if there's a conflict between different duties?",
        "start": 597780,
        "end": 600180,
        "speaker": "A"
      },
      {
        "sid": "s000101",
        "text": "I'm supposed to be respectful people and consider their feelings, but I'm also supposed to always be honest.",
        "start": 600420,
        "end": 605860,
        "speaker": "A"
      },
      {
        "sid": "s000102",
        "text": "That's pretty hard to rectify.",
        "start": 605860,
        "end": 607060,
        "speaker": "A"
      },
      {
        "sid": "s000103",
        "text": "You only want good intentions as opposed to good outcomes.",
        "start": 608980,
        "end": 611980,
        "speaker": "A"
      },
      {
        "sid": "s000104",
        "text": "It's very hard to have an ethical strategy where you ignore the outcomes as long as you've done a good intention.",
        "start": 611980,
        "end": 617710,
        "speaker": "A"
      },
      {
        "sid": "s000105",
        "text": "And there's really no definition of ethical behaviors at all.",
        "start": 618270,
        "end": 620910,
        "speaker": "A"
      },
      {
        "sid": "s000106",
        "text": "Questions on deontological thinking, especially Kant.",
        "start": 623790,
        "end": 626670,
        "speaker": "A"
      },
      {
        "sid": "s000107",
        "text": "So we see the play between the two, right?",
        "start": 629150,
        "end": 631310,
        "speaker": "A"
      },
      {
        "sid": "s000108",
        "text": "One is emphasizing the caliber of the actions that you take, and the other one is quantifying the results those actions have.",
        "start": 631310,
        "end": 637950,
        "speaker": "A"
      },
      {
        "sid": "s000109",
        "text": "All right, so the last one.",
        "start": 644120,
        "end": 644920,
        "speaker": "A"
      },
      {
        "sid": "s000110",
        "text": "And then I want to start doing some examples of applying these, and we'll talk through them.",
        "start": 644920,
        "end": 648120,
        "speaker": "A"
      },
      {
        "sid": "s000111",
        "text": "Is virtue ethics.",
        "start": 648600,
        "end": 649880,
        "speaker": "A"
      },
      {
        "sid": "s000112",
        "text": "Virtue ethics would be familiar to those who have looked at maybe Aristotle or Confucius.",
        "start": 651480,
        "end": 656440,
        "speaker": "A"
      },
      {
        "sid": "s000113",
        "text": "The idea is basically that a human character has some qualities, some virtues that make that up.",
        "start": 656760,
        "end": 664120,
        "speaker": "A"
      },
      {
        "sid": "s000114",
        "text": "And we can live happy and fulfilling life lives.",
        "start": 664520,
        "end": 667230,
        "speaker": "A"
      },
      {
        "sid": "s000115",
        "text": "If we try to follow certain principles that might be better for us.",
        "start": 667230,
        "end": 671830,
        "speaker": "A"
      },
      {
        "sid": "s000116",
        "text": "Just like exercise.",
        "start": 673030,
        "end": 674630,
        "speaker": "A"
      },
      {
        "sid": "s000117",
        "text": "You can practice that virtue over and over again and enhance your likelihood.",
        "start": 674790,
        "end": 678630,
        "speaker": "A"
      },
      {
        "sid": "s000118",
        "text": "So let's do examples.",
        "start": 683990,
        "end": 688150,
        "speaker": "A"
      },
      {
        "sid": "s000119",
        "text": "Confucius had and Aristotle both had their form of these ethics where the idea was to find a middle ground, a mean.",
        "start": 689430,
        "end": 698950,
        "speaker": "A"
      },
      {
        "sid": "s000120",
        "text": "Confucius called it a doctrine of mean.",
        "start": 699750,
        "end": 701630,
        "speaker": "A"
      },
      {
        "sid": "s000121",
        "text": "Aristotle called this golden mean.",
        "start": 701630,
        "end": 703510,
        "speaker": "A"
      },
      {
        "sid": "s000122",
        "text": "The idea was that you might have a complete lack or deficiency of a particular virtue and you might have an excess of that virtue.",
        "start": 705110,
        "end": 712390,
        "speaker": "A"
      },
      {
        "sid": "s000123",
        "text": "And you wanted to find the middle ground.",
        "start": 712390,
        "end": 714070,
        "speaker": "A"
      },
      {
        "sid": "s000124",
        "text": "If you consider bravery as an example, one thing to be completely reckless and jump into any situation, no matter the danger, versus being completely cowardly and not being able to embrace any risk at all.",
        "start": 715190,
        "end": 729120,
        "speaker": "A"
      },
      {
        "sid": "s000125",
        "text": "There should be some medium in between.",
        "start": 729600,
        "end": 731200,
        "speaker": "A"
      },
      {
        "sid": "s000126",
        "text": "Now, the problem is, as you might guess, there's a big line between cowardice and leftist.",
        "start": 732000,
        "end": 737520,
        "speaker": "A"
      },
      {
        "sid": "s000127",
        "text": "So how do you find that medium?",
        "start": 737840,
        "end": 739600,
        "speaker": "A"
      },
      {
        "sid": "s000128",
        "text": "Well, Aristotle, Confucius have their different approaches on how to do that.",
        "start": 740640,
        "end": 744400,
        "speaker": "A"
      },
      {
        "sid": "s000129",
        "text": "But the idea was that you had a variety of virtues that we could agree are good things that a person might have.",
        "start": 744850,
        "end": 750450,
        "speaker": "A"
      },
      {
        "sid": "s000130",
        "text": "Whether courage, friendliness, temperance, generosity, all these could be great examples.",
        "start": 750690,
        "end": 756050,
        "speaker": "A"
      },
      {
        "sid": "s000131",
        "text": "And the key is to try to find that midpoint between an excess and a deficiency that exists for that particular virtue.",
        "start": 756610,
        "end": 763730,
        "speaker": "A"
      },
      {
        "sid": "s000132",
        "text": "My brother in law was trying to argue this point with me that there's always a middle, middle ground between any two points of any argument, and that's where we should always go.",
        "start": 766290,
        "end": 773610,
        "speaker": "A"
      },
      {
        "sid": "s000133",
        "text": "And so of course I went to the trees with him on certain things and he realized that's a terrible argument.",
        "start": 773850,
        "end": 778130,
        "speaker": "A"
      },
      {
        "sid": "s000134",
        "text": "But the point is, at certain times it does make sense.",
        "start": 778130,
        "end": 781130,
        "speaker": "A"
      },
      {
        "sid": "s000135",
        "text": "And in this case, what they're pushing is trying to find a middle ground when looking at virtues between excess and divisions.",
        "start": 782730,
        "end": 791770,
        "speaker": "A"
      },
      {
        "sid": "s000136",
        "text": "A long time ago, I played a game as a kid called Ultima.",
        "start": 794090,
        "end": 797660,
        "speaker": "A"
      },
      {
        "sid": "s000137",
        "text": "This is really showing my age here.",
        "start": 797660,
        "end": 799140,
        "speaker": "A"
      },
      {
        "sid": "s000138",
        "text": "Ultima was like one of the early RPGs and the guy who made Ultima, it was a fantasy world where there were cities and villages and dungeons that you could explore and all that stuff.",
        "start": 799940,
        "end": 811220,
        "speaker": "A"
      },
      {
        "sid": "s000139",
        "text": "But the guy who built this game put himself in the game.",
        "start": 811619,
        "end": 814900,
        "speaker": "A"
      },
      {
        "sid": "s000140",
        "text": "He put himself as a character in the game.",
        "start": 814900,
        "end": 816740,
        "speaker": "A"
      },
      {
        "sid": "s000141",
        "text": "And the best way that he found people would win this game is by razing villages that were supposed to be peaceful, killing off people and grabbing their stuff, looting his own castle, all this stuff.",
        "start": 816900,
        "end": 828130,
        "speaker": "A"
      },
      {
        "sid": "s000142",
        "text": "And he realized this is not really inspiring people to be virtuous.",
        "start": 828290,
        "end": 832570,
        "speaker": "A"
      },
      {
        "sid": "s000143",
        "text": "Can I do better?",
        "start": 832570,
        "end": 833410,
        "speaker": "A"
      },
      {
        "sid": "s000144",
        "text": "Why am I giving people this game where the best way to win is to behave in your worst possible way?",
        "start": 833730,
        "end": 840450,
        "speaker": "A"
      },
      {
        "sid": "s000145",
        "text": "So he tried, and you can see the remnants of his left to.",
        "start": 840770,
        "end": 844930,
        "speaker": "A"
      },
      {
        "sid": "s000146",
        "text": "Instead of just, let's just win the game by Whatever means possible.",
        "start": 844930,
        "end": 848050,
        "speaker": "A"
      },
      {
        "sid": "s000147",
        "text": "He actually embraced virtues.",
        "start": 848600,
        "end": 850000,
        "speaker": "A"
      },
      {
        "sid": "s000148",
        "text": "This guy came up with a virtue system for his game.",
        "start": 850000,
        "end": 852360,
        "speaker": "A"
      },
      {
        "sid": "s000149",
        "text": "He had honesty, compassion, valor, justice, sacrifice, honor, spirituality, humility.",
        "start": 852680,
        "end": 856920,
        "speaker": "A"
      },
      {
        "sid": "s000150",
        "text": "He would judge how well you performed in all these throughout the game.",
        "start": 857080,
        "end": 860440,
        "speaker": "A"
      },
      {
        "sid": "s000151",
        "text": "Did you run away from fights that wouldn't be very honorable?",
        "start": 861960,
        "end": 864560,
        "speaker": "A"
      },
      {
        "sid": "s000152",
        "text": "Did you stand up for those in need?",
        "start": 864560,
        "end": 866040,
        "speaker": "A"
      },
      {
        "sid": "s000153",
        "text": "That would be great.",
        "start": 866680,
        "end": 867440,
        "speaker": "A"
      },
      {
        "sid": "s000154",
        "text": "You could give blood in this game.",
        "start": 867440,
        "end": 868760,
        "speaker": "A"
      },
      {
        "sid": "s000155",
        "text": "It was crazy, right?",
        "start": 868760,
        "end": 869760,
        "speaker": "A"
      },
      {
        "sid": "s000156",
        "text": "So it was like an actual attempt to make virtue the, the end point of the game.",
        "start": 869760,
        "end": 875330,
        "speaker": "A"
      },
      {
        "sid": "s000157",
        "text": "And then he went to other games where he said, what if virtue went too far?",
        "start": 875330,
        "end": 878530,
        "speaker": "A"
      },
      {
        "sid": "s000158",
        "text": "Right?",
        "start": 878930,
        "end": 879330,
        "speaker": "A"
      },
      {
        "sid": "s000159",
        "text": "Either give to the poor or you'll have nothing.",
        "start": 879410,
        "end": 881730,
        "speaker": "A"
      },
      {
        "sid": "s000160",
        "text": "And then what if you had to look at two virtues from different civilizations and try to see how they interface?",
        "start": 882530,
        "end": 887570,
        "speaker": "A"
      },
      {
        "sid": "s000161",
        "text": "It was pretty cool.",
        "start": 887570,
        "end": 888450,
        "speaker": "A"
      },
      {
        "sid": "s000162",
        "text": "Now you can see the same kinds of virtue systems in games like Baldur's Gate or something else where they're looking at how your decisions affect other characters.",
        "start": 889010,
        "end": 898570,
        "speaker": "A"
      },
      {
        "sid": "s000163",
        "text": "But this was the, the first one that I saw that was canonical as far as trying to actually employ virtue ethics.",
        "start": 898570,
        "end": 904140,
        "speaker": "A"
      },
      {
        "sid": "s000164",
        "text": "Okay, so virtue ethics is pretty cool in that you really do get a set of virtues that make sense, that might be aspirational, that might guide how you behave, that you can think about how you behave.",
        "start": 905100,
        "end": 918860,
        "speaker": "A"
      },
      {
        "sid": "s000165",
        "text": "I want to be honest, I want to be compassionate.",
        "start": 919020,
        "end": 921180,
        "speaker": "A"
      },
      {
        "sid": "s000166",
        "text": "But at the same time, finding that middle ground is challenging.",
        "start": 921740,
        "end": 924460,
        "speaker": "A"
      },
      {
        "sid": "s000167",
        "text": "And being able to know decision between two middle pieces.",
        "start": 925270,
        "end": 929830,
        "speaker": "A"
      },
      {
        "sid": "s000168",
        "text": "It's one thing to say, oh, I don't want to be cowardly, but it's another thing to say, am I being brave enough or too brave?",
        "start": 930070,
        "end": 935670,
        "speaker": "A"
      },
      {
        "sid": "s000169",
        "text": "And so those kind of fine decisions are difficult to make in virtue.",
        "start": 935990,
        "end": 939510,
        "speaker": "A"
      },
      {
        "sid": "s000170",
        "text": "It's also hard to have principles that are cross cutting across cultures.",
        "start": 942070,
        "end": 946470,
        "speaker": "A"
      },
      {
        "sid": "s000171",
        "text": "And so this is also somewhat challenging.",
        "start": 948390,
        "end": 950170,
        "speaker": "A"
      },
      {
        "sid": "s000172",
        "text": "But there's a clear difference.",
        "start": 951360,
        "end": 952440,
        "speaker": "A"
      },
      {
        "sid": "s000173",
        "text": "At least he's not so clear.",
        "start": 952440,
        "end": 954840,
        "speaker": "A"
      },
      {
        "sid": "s000174",
        "text": "But there is a difference between this and the ontological thinking.",
        "start": 954840,
        "end": 957400,
        "speaker": "A"
      },
      {
        "sid": "s000175",
        "text": "The ontological thinking is, let me think of imperatives that are good in general, whereas virtue ethics is.",
        "start": 957400,
        "end": 962880,
        "speaker": "A"
      },
      {
        "sid": "s000176",
        "text": "These are things that I want to be as parts of my character.",
        "start": 962960,
        "end": 967200,
        "speaker": "A"
      },
      {
        "sid": "s000177",
        "text": "And I'm going to make decisions that work with that.",
        "start": 967200,
        "end": 970320,
        "speaker": "A"
      },
      {
        "sid": "s000178",
        "text": "Can I lie and still improve my compassion?",
        "start": 970960,
        "end": 974000,
        "speaker": "A"
      },
      {
        "sid": "s000179",
        "text": "Yeah, I might be lying and going against the categorical imperative, but I'm lying for the purposes of compassion.",
        "start": 974320,
        "end": 980800,
        "speaker": "A"
      },
      {
        "sid": "s000180",
        "text": "Does that make sense?",
        "start": 980800,
        "end": 981680,
        "speaker": "A"
      },
      {
        "sid": "s000181",
        "text": "Who knew we'd talk about older women?",
        "start": 988240,
        "end": 989840,
        "speaker": "A"
      },
      {
        "sid": "s000182",
        "text": "Okay, great.",
        "start": 990000,
        "end": 990720,
        "speaker": "A"
      },
      {
        "sid": "s000183",
        "text": "So there's a whole spectrum of different types of ethical ideologies.",
        "start": 992240,
        "end": 997040,
        "speaker": "A"
      },
      {
        "sid": "s000184",
        "text": "And this is one attempt to try to canonize them into a.",
        "start": 997200,
        "end": 1001200,
        "speaker": "A"
      },
      {
        "sid": "s000185",
        "text": "This is an old paper, but the idea was that there are two dimensions here.",
        "start": 1003210,
        "end": 1011530,
        "speaker": "A"
      },
      {
        "sid": "s000186",
        "text": "One of them is relativism.",
        "start": 1011610,
        "end": 1012970,
        "speaker": "A"
      },
      {
        "sid": "s000187",
        "text": "And the Idea here is that high relativism means that you're going to reject the possibility of relying on kind of these universal or moral rules.",
        "start": 1013530,
        "end": 1024170,
        "speaker": "A"
      },
      {
        "sid": "s000188",
        "text": "You're going to say that there's.",
        "start": 1025290,
        "end": 1027700,
        "speaker": "A"
      },
      {
        "sid": "s000189",
        "text": "It's not relative to the situation or different things.",
        "start": 1027860,
        "end": 1031940,
        "speaker": "A"
      },
      {
        "sid": "s000190",
        "text": "Oh, sorry, that.",
        "start": 1032020,
        "end": 1032820,
        "speaker": "A"
      },
      {
        "sid": "s000191",
        "text": "That I was doing low relativism.",
        "start": 1032820,
        "end": 1035860,
        "speaker": "A"
      },
      {
        "sid": "s000192",
        "text": "High relativism is.",
        "start": 1036020,
        "end": 1037300,
        "speaker": "A"
      },
      {
        "sid": "s000193",
        "text": "It is not going to be a universal moral rule.",
        "start": 1037540,
        "end": 1039980,
        "speaker": "A"
      },
      {
        "sid": "s000194",
        "text": "It's going to be relative to the situation and what's happening at that point in time.",
        "start": 1039980,
        "end": 1044340,
        "speaker": "A"
      },
      {
        "sid": "s000195",
        "text": "This is not something that Kant would have embraced.",
        "start": 1044340,
        "end": 1046580,
        "speaker": "A"
      },
      {
        "sid": "s000196",
        "text": "He did not want things that were relativism.",
        "start": 1047220,
        "end": 1049140,
        "speaker": "A"
      },
      {
        "sid": "s000197",
        "text": "Low relativism is that you use moral absolute.",
        "start": 1050260,
        "end": 1052860,
        "speaker": "A"
      },
      {
        "sid": "s000198",
        "text": "You should be trying to judge things based on scenarios that can or inputs that can apply universally.",
        "start": 1053490,
        "end": 1060450,
        "speaker": "A"
      },
      {
        "sid": "s000199",
        "text": "The other axis here is idealism.",
        "start": 1062930,
        "end": 1065410,
        "speaker": "A"
      },
      {
        "sid": "s000200",
        "text": "So if you have the right action, you can always get the right outcome.",
        "start": 1065890,
        "end": 1070610,
        "speaker": "A"
      },
      {
        "sid": "s000201",
        "text": "That's highly idealistic.",
        "start": 1070610,
        "end": 1072130,
        "speaker": "A"
      },
      {
        "sid": "s000202",
        "text": "Low idealism is, look, you're going to have sometimes good good outcomes and bad outcomes if you try to do the right action.",
        "start": 1072850,
        "end": 1082350,
        "speaker": "A"
      },
      {
        "sid": "s000203",
        "text": "That's just the nature of how actions work.",
        "start": 1082430,
        "end": 1084750,
        "speaker": "A"
      },
      {
        "sid": "s000204",
        "text": "So in this case, the ones we've described are both low relativism.",
        "start": 1087070,
        "end": 1092590,
        "speaker": "A"
      },
      {
        "sid": "s000205",
        "text": "They're not really situationally specific, but they differ in their idealism.",
        "start": 1093070,
        "end": 1098030,
        "speaker": "A"
      },
      {
        "sid": "s000206",
        "text": "Deontology and to some extent virtue ethics are high ideals.",
        "start": 1098270,
        "end": 1101950,
        "speaker": "A"
      },
      {
        "sid": "s000207",
        "text": "Low relativism, but consequentialism, utilitarianism is low ideals and low relativity.",
        "start": 1102510,
        "end": 1109640,
        "speaker": "A"
      },
      {
        "sid": "s000208",
        "text": "Does that make sense?",
        "start": 1109640,
        "end": 1110440,
        "speaker": "A"
      },
      {
        "sid": "s000209",
        "text": "So there are other ones out there.",
        "start": 1114200,
        "end": 1116440,
        "speaker": "A"
      },
      {
        "sid": "s000210",
        "text": "There's situational ethics, which is based completely on what you can do to embrace love.",
        "start": 1116840,
        "end": 1123080,
        "speaker": "A"
      },
      {
        "sid": "s000211",
        "text": "I won't make you learn this one for this particular class, but this is a high relativism.",
        "start": 1124920,
        "end": 1129400,
        "speaker": "A"
      },
      {
        "sid": "s000212",
        "text": "So really specific to the situation, what choice you make to embrace more love.",
        "start": 1129400,
        "end": 1134370,
        "speaker": "A"
      },
      {
        "sid": "s000213",
        "text": "And then sort of the other outcome of that is ethical egoism.",
        "start": 1136130,
        "end": 1140010,
        "speaker": "A"
      },
      {
        "sid": "s000214",
        "text": "Just do whatever you want with your own self interest.",
        "start": 1140010,
        "end": 1142050,
        "speaker": "A"
      },
      {
        "sid": "s000215",
        "text": "Another approach where idealism is low.",
        "start": 1142770,
        "end": 1145650,
        "speaker": "A"
      },
      {
        "sid": "s000216",
        "text": "Sometimes the outcome's gonna be good, sometimes it's bad, but just do what you want.",
        "start": 1146370,
        "end": 1149410,
        "speaker": "A"
      },
      {
        "sid": "s000217",
        "text": "All right, so let's.",
        "start": 1152290,
        "end": 1153730,
        "speaker": "A"
      },
      {
        "sid": "s000218",
        "text": "Let's try the three outcome.",
        "start": 1154210,
        "end": 1155490,
        "speaker": "A"
      },
      {
        "sid": "s000219",
        "text": "There's been a lot of iteration.",
        "start": 1157640,
        "end": 1158840,
        "speaker": "A"
      },
      {
        "sid": "s000220",
        "text": "This is where I will go a little off topic and then I'll get back to computer science.",
        "start": 1158920,
        "end": 1161920,
        "speaker": "A"
      },
      {
        "sid": "s000221",
        "text": "But I wanted to do one off topic that sort of cements our understanding of these three principles.",
        "start": 1161920,
        "end": 1166680,
        "speaker": "A"
      },
      {
        "sid": "s000222",
        "text": "There's been a lot of iterations of Robin Hood over the years.",
        "start": 1167880,
        "end": 1170360,
        "speaker": "A"
      },
      {
        "sid": "s000223",
        "text": "I don't think I like the ninja one as much, but it's okay.",
        "start": 1171559,
        "end": 1174040,
        "speaker": "A"
      },
      {
        "sid": "s000224",
        "text": "But the basic idea is that he favors the redistribution of the will through means that might considered legal.",
        "start": 1174840,
        "end": 1182660,
        "speaker": "A"
      },
      {
        "sid": "s000225",
        "text": "He's highly skilled and he's courageous in crafty.",
        "start": 1183140,
        "end": 1186820,
        "speaker": "A"
      },
      {
        "sid": "s000226",
        "text": "So if we take that as the representation of Robin Hood.",
        "start": 1188020,
        "end": 1191540,
        "speaker": "A"
      },
      {
        "sid": "s000227",
        "text": "My question to you is, this is where I got my picture from.",
        "start": 1192100,
        "end": 1195380,
        "speaker": "A"
      },
      {
        "sid": "s000228",
        "text": "How would you judge him?",
        "start": 1196420,
        "end": 1197619,
        "speaker": "A"
      },
      {
        "sid": "s000229",
        "text": "So let me ask, if you had to judge him from a deontological perspective, right.",
        "start": 1198020,
        "end": 1202940,
        "speaker": "A"
      },
      {
        "sid": "s000230",
        "text": "You're someone who steals from the rich, gives the needy breaking the law through ostensibly violent means, what would you say?",
        "start": 1202940,
        "end": 1214610,
        "speaker": "A"
      },
      {
        "sid": "s000231",
        "text": "Deontological, Kantian ethics.",
        "start": 1216930,
        "end": 1220290,
        "speaker": "A"
      },
      {
        "sid": "s000232",
        "text": "What would Kant say?",
        "start": 1220690,
        "end": 1221730,
        "speaker": "A"
      },
      {
        "sid": "s000233",
        "text": "Yeah, I don't think like Kant would approve because he's like doing things.",
        "start": 1223490,
        "end": 1227730,
        "speaker": "B"
      },
      {
        "sid": "s000234",
        "text": "Because Kant wasn't considered like that, wasn't considering the relative impact like he was like talking about, about in the general case.",
        "start": 1228050,
        "end": 1233650,
        "speaker": "B"
      },
      {
        "sid": "s000235",
        "text": "In this case, if you can judge in general violent acts, he wouldn't consider it as a means justify the end.",
        "start": 1234690,
        "end": 1243410,
        "speaker": "B"
      },
      {
        "sid": "s000236",
        "text": "Right, right.",
        "start": 1243810,
        "end": 1245170,
        "speaker": "A"
      },
      {
        "sid": "s000237",
        "text": "He's using violence as a tool to achieve what you might consider a noble outcome.",
        "start": 1245330,
        "end": 1250450,
        "speaker": "A"
      },
      {
        "sid": "s000238",
        "text": "But Kant would probably say you are breaking the law by stealing.",
        "start": 1250610,
        "end": 1254210,
        "speaker": "A"
      },
      {
        "sid": "s000239",
        "text": "And maybe Kant would treat the fact that we shouldn't steal as some.",
        "start": 1254210,
        "end": 1258610,
        "speaker": "A"
      },
      {
        "sid": "s000240",
        "text": "That is a universal categorical imperative that we should all be applying.",
        "start": 1258840,
        "end": 1264840,
        "speaker": "A"
      },
      {
        "sid": "s000241",
        "text": "Everybody see that?",
        "start": 1266760,
        "end": 1267640,
        "speaker": "A"
      },
      {
        "sid": "s000242",
        "text": "Good.",
        "start": 1268120,
        "end": 1268520,
        "speaker": "A"
      },
      {
        "sid": "s000243",
        "text": "How about from a utilitarian perspective?",
        "start": 1270200,
        "end": 1272880,
        "speaker": "A"
      },
      {
        "sid": "s000244",
        "text": "I would be Judge Robertson.",
        "start": 1272880,
        "end": 1274200,
        "speaker": "A"
      },
      {
        "sid": "s000245",
        "text": "Yes.",
        "start": 1276120,
        "end": 1276600,
        "speaker": "A"
      },
      {
        "sid": "s000246",
        "text": "You can argue the four that you.",
        "start": 1276760,
        "end": 1278720,
        "speaker": "C"
      },
      {
        "sid": "s000247",
        "text": "Give are becoming happier.",
        "start": 1278720,
        "end": 1280840,
        "speaker": "B"
      },
      {
        "sid": "s000248",
        "text": "They're not so happy about it.",
        "start": 1283480,
        "end": 1284700,
        "speaker": "A"
      },
      {
        "sid": "s000249",
        "text": "Right.",
        "start": 1285330,
        "end": 1285570,
        "speaker": "A"
      },
      {
        "sid": "s000250",
        "text": "So how would we run that algebraically?",
        "start": 1286130,
        "end": 1288610,
        "speaker": "A"
      },
      {
        "sid": "s000251",
        "text": "Some might say, well, the wealthy that he's stealing from have plenty to give or just one individual and he's distributing among many.",
        "start": 1288610,
        "end": 1294770,
        "speaker": "A"
      },
      {
        "sid": "s000252",
        "text": "And so in that case, the happiness that it would have for someone who has nothing to have what he's able to give them is more than the pain that it costs that wealthier person.",
        "start": 1295170,
        "end": 1304930,
        "speaker": "A"
      },
      {
        "sid": "s000253",
        "text": "It's possible.",
        "start": 1305330,
        "end": 1305970,
        "speaker": "A"
      },
      {
        "sid": "s000254",
        "text": "Any other comments about that one?",
        "start": 1307810,
        "end": 1309170,
        "speaker": "A"
      },
      {
        "sid": "s000255",
        "text": "Yeah, again, I'm not arguing for this, but you might say a utilitarian like Mills might have said there's a limit to how much one person needs for happiness, and therefore the amount that he's taking really doesn't impact very much on their real happiness quotient.",
        "start": 1309890,
        "end": 1326140,
        "speaker": "A"
      },
      {
        "sid": "s000256",
        "text": "But it would help a lot to the poor, so they might judge him positively.",
        "start": 1326220,
        "end": 1331660,
        "speaker": "A"
      },
      {
        "sid": "s000257",
        "text": "How about Virtue X?",
        "start": 1332700,
        "end": 1333900,
        "speaker": "A"
      },
      {
        "sid": "s000258",
        "text": "Any ideas on that one?",
        "start": 1334540,
        "end": 1335660,
        "speaker": "A"
      },
      {
        "sid": "s000259",
        "text": "Is he finding a happy medium between robbing and not robbing charity?",
        "start": 1341510,
        "end": 1348470,
        "speaker": "A"
      },
      {
        "sid": "s000260",
        "text": "I mean, you could say he's showing things.",
        "start": 1350150,
        "end": 1351910,
        "speaker": "A"
      },
      {
        "sid": "s000261",
        "text": "He's courageous enough to act on principles, but he's not reckless.",
        "start": 1352070,
        "end": 1355110,
        "speaker": "A"
      },
      {
        "sid": "s000262",
        "text": "He doesn't put himself too much in harm's way.",
        "start": 1355110,
        "end": 1357030,
        "speaker": "A"
      },
      {
        "sid": "s000263",
        "text": "He's merciful, but still tough on people who practice excess.",
        "start": 1357430,
        "end": 1360190,
        "speaker": "A"
      },
      {
        "sid": "s000264",
        "text": "He's generous to give stolen and gives to the poor instead of keeping it for himself.",
        "start": 1360190,
        "end": 1363680,
        "speaker": "A"
      },
      {
        "sid": "s000265",
        "text": "So there are virtuous things that you can see in his character and all that you can say that he is doing things From a virtue ethic standpoint.",
        "start": 1364160,
        "end": 1375760,
        "speaker": "A"
      },
      {
        "sid": "s000266",
        "text": "Any questions on any of that so far?",
        "start": 1377920,
        "end": 1379600,
        "speaker": "A"
      },
      {
        "sid": "s000267",
        "text": "So I'm curious, given all of that, what would you say in this particular case?",
        "start": 1383040,
        "end": 1390850,
        "speaker": "A"
      },
      {
        "sid": "s000268",
        "text": "You've got three ethical frameworks that look, at the end of the day, these are just frameworks for thinking about a problem.",
        "start": 1390850,
        "end": 1396850,
        "speaker": "A"
      },
      {
        "sid": "s000269",
        "text": "They aren't the end solution.",
        "start": 1396850,
        "end": 1398130,
        "speaker": "A"
      },
      {
        "sid": "s000270",
        "text": "You yourself will have a decision that you'll make based on all these different ethical frameworks and on the information that's actually relevant in here.",
        "start": 1398130,
        "end": 1405650,
        "speaker": "A"
      },
      {
        "sid": "s000271",
        "text": "This is just a way to try to formula.",
        "start": 1405650,
        "end": 1407210,
        "speaker": "A"
      },
      {
        "sid": "s000272",
        "text": "So just for fun, how many say, hey, very nice, very nice.",
        "start": 1408330,
        "end": 1416020,
        "speaker": "A"
      },
      {
        "sid": "s000273",
        "text": "How many say B?",
        "start": 1416020,
        "end": 1417060,
        "speaker": "A"
      },
      {
        "sid": "s000274",
        "text": "Okay, some utilitarians here.",
        "start": 1418180,
        "end": 1420100,
        "speaker": "A"
      },
      {
        "sid": "s000275",
        "text": "Great.",
        "start": 1420100,
        "end": 1420420,
        "speaker": "B"
      },
      {
        "sid": "s000276",
        "text": "C. Yeah, that's where I fall.",
        "start": 1421220,
        "end": 1424100,
        "speaker": "A"
      },
      {
        "sid": "s000277",
        "text": "How about D?",
        "start": 1425380,
        "end": 1426260,
        "speaker": "A"
      },
      {
        "sid": "s000278",
        "text": "Then you should go watch the movie.",
        "start": 1427300,
        "end": 1428500,
        "speaker": "A"
      },
      {
        "sid": "s000279",
        "text": "All right, and then E. Good to know.",
        "start": 1428900,
        "end": 1434300,
        "speaker": "A"
      },
      {
        "sid": "s000280",
        "text": "Okay, let's try something else.",
        "start": 1434300,
        "end": 1436740,
        "speaker": "A"
      },
      {
        "sid": "s000281",
        "text": "Let's get to something else.",
        "start": 1436980,
        "end": 1437940,
        "speaker": "A"
      },
      {
        "sid": "s000282",
        "text": "Computing.",
        "start": 1438170,
        "end": 1438650,
        "speaker": "A"
      },
      {
        "sid": "s000283",
        "text": "So let's say that we have some social media app that's collecting our data and there's so many positives to that.",
        "start": 1439130,
        "end": 1446890,
        "speaker": "A"
      }
    ],
    "context_after_main_text": [
      {
        "sid": "s000284",
        "text": "Oh, wow.",
        "start": 1446890,
        "end": 1447610,
        "speaker": "A"
      },
      {
        "sid": "s000285",
        "text": "Collecting my data means that it will be able to inform me of better choices.",
        "start": 1447610,
        "end": 1451250,
        "speaker": "A"
      },
      {
        "sid": "s000286",
        "text": "It can talk to ads, it can track my purchases.",
        "start": 1451250,
        "end": 1454730,
        "speaker": "A"
      },
      {
        "sid": "s000287",
        "text": "So exciting.",
        "start": 1454970,
        "end": 1455690,
        "speaker": "A"
      },
      {
        "sid": "s000288",
        "text": "But aside from those pros, there are some invasion of my privacy, potential for misunderstood misuse.",
        "start": 1457450,
        "end": 1464450,
        "speaker": "A"
      },
      {
        "sid": "s000289",
        "text": "They're going to report what I'm buying.",
        "start": 1464690,
        "end": 1466450,
        "speaker": "A"
      },
      {
        "sid": "s000290",
        "text": "How do we judge the ethics of this collection?",
        "start": 1468050,
        "end": 1470130,
        "speaker": "A"
      },
      {
        "sid": "s000291",
        "text": "So for this data collection, what would you say in terms of ethical frameworks?",
        "start": 1470770,
        "end": 1476570,
        "speaker": "A"
      },
      {
        "sid": "s000292",
        "text": "Do you think this is good or bad?",
        "start": 1476570,
        "end": 1478010,
        "speaker": "A"
      },
      {
        "sid": "s000293",
        "text": "Any comments?",
        "start": 1478010,
        "end": 1478610,
        "speaker": "A"
      }
    ]
  }
]
## The "King Midas Problem" of Direct Value Alignment

The challenge of "*directly uploading specific norms*" into an AI is vividly illustrated by the ancient Greek myth of King Midas. Midas wished for everything he touched to turn to gold, a seemingly perfect value, but this direct specification led to unintended, catastrophic consequencesâ€”he couldn't eat or drink, and even his daughter turned to gold. This classic "be careful what you wish for" scenario highlights *why indirect normativity is needed*: precisely defining human values in a way that avoids perverse instantiations or unforeseen negative outcomes is incredibly difficult, making a "direct assault" on value alignment problematic for an **Artificial Superintelligence (ASI)**.

This mythological tale underscores the core problem that indirect normativity seeks to solve: our inability to perfectly articulate our values without ambiguity or loopholes. If we cannot successfully upload specific norms directly, then an **indirect strategy** becomes essential, guiding the AI towards what we *truly* want, rather than what we *literally* say, much like Midas would have wanted the *spirit* of wealth, not its destructive literal interpretation.

## Yudkowsky's "Coherent Extrapolated Volition" as an Indirect Path

The concept of an "*indirect strategy*" for value alignment is famously exemplified by **Yudkowsky**'s proposal of **Coherent Extrapolated Volition (CEV)**. Instead of attempting the impossible task of *directly uploading specific norms* into a **seed AI**, CEV suggests that an ASI should be designed to infer and implement what humanity *would* want if we were smarter, more knowledgeable, and had resolved all our internal conflicts and inconsistencies. This is a prime example of an **alternative method for guiding AI values**.

CEV represents a sophisticated form of **indirect normativity** because it doesn't dictate values upfront but rather sets up a process for the AI to discover and extrapolate them from human behavior and preferences. It acknowledges that a "direct assault" on value alignment is not viable, proposing that the AI should act as a benevolent interpreter of our "true" desires, thereby aligning its goals with a more ideal, coherent version of human will, rather than a flawed, directly specified set of rules.
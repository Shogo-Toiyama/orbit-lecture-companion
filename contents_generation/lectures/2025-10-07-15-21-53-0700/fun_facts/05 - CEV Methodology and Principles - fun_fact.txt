## The "Genie Problem": Making Humanity's Wishes Coherent

The core idea of CEV as an *extrapolated version* of human values, derived from diverse *ethical systems*, directly tackles what's often called the "Genie Problem" in AI safety. Imagine you have a genie that grants wishes, but it interprets them literally, often leading to disastrous unintended consequences because human wishes are frequently vague, contradictory, or poorly thought out. The "Genie Problem" highlights the immense difficulty of articulating human desires and values perfectly, without ambiguity or potential for misinterpretation.

CEV attempts to solve this by not asking for a direct "wish" from a single human, but by having the ASI *extrapolate* what humanity *would* wish for if it were perfectly rational, informed, and coherent, thus creating an *abstract generality* of values. This indirect method is proposed as a *better way* than trying to "upload values" directly, precisely because it acknowledges the incoherence and contradictions within individual human values. By creating a *baseline* of diverse ethical systems and then letting the ASI find the "coherent" extrapolation, it aims to arrive at a set of values that truly represent humanity's best interests, ensuring a *trustworthy and safe ASI* that doesn't "go its own way."

## The Oracle AI: Asking "What Would Humanity Want?"

The *heuristic principle* of CEV, where the system will *defer to the ASI* regarding how to apply generalized human values, echoes a critical concept in AI safety: the Oracle AI. An Oracle AI is a hypothetical superintelligence designed to answer complex questions or provide guidance without directly interacting with the world or taking actions. This aligns perfectly with CEV's aim to have the ASI interpret and apply abstract values, essentially "answering" how to act in a value-aligned way, rather than giving it free rein to implement its own interpretation.

This approach is a *feasible way* to inform the computer about human values because it offloads the intricate work of specific value application to the ASI, much like an oracle provides guidance. However, the "oracle" must first understand the *abstract general idea* of human values, which is the difficult part of *implantation*, ensuring its extrapolations are truly coherent with humanity's deepest desires. The ultimate goal is to *keep human beings in the loop* by ensuring the ASI's "answers" (its applications) are always beneficial and aligned with humanity's extrapolated volition.
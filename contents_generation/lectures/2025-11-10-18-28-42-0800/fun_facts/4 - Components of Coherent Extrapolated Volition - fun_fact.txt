## The "Bitter Lesson" of AI and CEV's Indirect Wisdom

The concept of Coherent Extrapolated Volition (CEV) as an *indirect method* for instilling human values in an Artificial Superintelligence (ASI) finds a surprising parallel in a foundational insight from AI research known as the "bitter lesson." This lesson, which emerged from decades of AI development, highlights that attempts to directly encode human knowledge or specific rules into AI systems consistently fail to scale or generalize effectively. Instead, the lecture notes that "building general learning systems that can learn for themselves from data" [cite: s000128, s000133, s000134, s000136] proves far more robust.

CEV embodies this bitter lesson by rejecting the *impossible task of directly uploading specific values* [cite: s000029, s000053, s000072] and instead offloading the complex work of understanding and applying values to the ASI itself [cite: s000057]. Rather than providing rigid, explicit rules that are "prone to misinterpretation" [cite: s000088], CEV offers an *open-ended* and abstract framework, allowing the ASI to *extrapolate a generalized understanding of human values* [cite: s000061] and invent its own "rules" based on underlying meaning. This approach aligns perfectly with the empirical evidence that general learning mechanisms, rather than hand-coded specifics, are the path to powerful and adaptable AI.

## The "Envelope" of Values and the Challenge of Tacit Knowledge

The lecture uses a vivid analogy to explain the *extrapolated* component of CEV, likening it to "putting human values in an envelope and asking the ASI to 'guess what's there'" [cite: s000063]. This seemingly simple image points to a profound challenge in AI alignment: the problem of **tacit knowledge**. Tacit knowledge refers to the kind of understanding we possess but find incredibly difficult, if not impossible, to articulate explicitly or translate into formal rules. For instance, we know how to ride a bike, but writing down every micro-adjustment of balance and force is nearly impossible.

This difficulty is precisely why *humans cannot definitively decide what those specific values ought to be* [cite: s000054] and why it would be "extremely difficult to translate them into computer language" [cite: s000055]. CEV's indirect approach, by asking the ASI to *extrapolate an abstract generality of human values* [cite: s000061] from a broad baseline, bypasses the need for humans to perfectly articulate the inarticulable. It's an elegant solution to the "do as I mean, not as I say" [cite: s000112, s000113] problem, allowing the ASI to grasp the underlying intent and meaning of human values, even when we can't fully open the "envelope" ourselves.
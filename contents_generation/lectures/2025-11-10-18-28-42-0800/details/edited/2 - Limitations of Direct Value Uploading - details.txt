# 2. Limitations of Direct Value Uploading

This topic explores the significant challenges and ultimate impossibility of directly uploading specific human values into an Artificial Superintelligence (ASI). It highlights that despite various methods considered, attempts to directly program values have proven unsafe and unfeasible, necessitating alternative, indirect approaches to ensure ASI remains aligned with human interests.

## üö´ The Impossibility of Direct Value Upload

Previously, the assumption was that if humans knew what values they desired, these could be directly uploaded into a seed computer as it developed into an ASI ‚ü¶s000011, s000012‚üß.
However, this assumption is considered a significant and ultimately unrealizable "if" ‚ü¶s000011‚üß.
The conclusion reached is that there is currently no safe method, and likely never will be, for directly uploading specific values into the seed computer that becomes an ASI ‚ü¶s000015‚üß.

## üöß Specific Obstacles to Direct Value Uploading

Directly uploading specific values is impossible due to two main difficulties:
* Humans themselves cannot definitively agree upon or decide what those values ought to be.
* Even if these values were known, translating them into computer language and effectively conveying them to the computer would be extremely challenging.
‚ü¶s000053, s000054, s000055 (noted elsewhere in the lecture)‚üß
More broadly, the "bitter lesson of AI" suggests that attempting to embed human knowledge or rules directly into systems generally does not work ‚ü¶s000126, s000127, s000136 (noted elsewhere in the lecture)‚üß.

## üîç Critique of Direct Value Uploading Methods

Various direct methods for uploading values have been explored and subsequently critiqued, including concepts such as *wire heading* and *mine crimes* ‚ü¶s000014‚üß.
Other techniques like *reinforcement learning* and *scaffolding* have also been considered.
However, the possibilities for direct value uploading appear to have been exhausted without yielding a viable solution ‚ü¶s000019‚üß.

### ‚ùå Problems with Rule-Based Approaches

A significant problem with using rules, which is a form of direct instruction, is their inherent openness to interpretation ‚ü¶s000087, s000101 (noted elsewhere in the lecture)‚üß.
Rule-following is difficult to implement in systems and does not accurately reflect how human beings operate in the world ‚ü¶s000088, s000091, s000092, s000093 (noted elsewhere in the lecture)‚üß.
This leads to situations where the intended meaning behind a rule is easily misinterpreted, resulting in actions that differ from what was desired ‚ü¶s000103, s000105, s000106 (noted elsewhere in the lecture)‚üß.

## ‚ú® The Need for an Alternative Approach

Given the failure of all direct methods, a critical question arises: how can the friendliness of an ASI be assured if direct value uploading is impossible? ‚ü¶s000016, s000017, s000018‚üß.
One potential conclusion from this impasse might be to halt ASI development due to its inherent dangers ‚ü¶s000022, s000023, s000024‚üß.
However, the author being discussed does not advocate for stopping development but instead seeks an alternative solution ‚ü¶s000025, s000026‚üß.
Since specific norms cannot be directly implanted, an *indirect* approach becomes necessary ‚ü¶s000027, s000028 (noted elsewhere in the lecture)‚üß, leading to the concept of *indirect normativity* ‚ü¶s000029, s000030 (noted elsewhere in the lecture)‚üß.
This indirect method is considered a feasible way to inform the seed computer about what human values might entail ‚ü¶s000073 (noted elsewhere in the lecture)‚üß.

## üí° Supplement: Understanding "Friendly AI"

The term "**friendly AI**" was coined by Eliezer Yudkowsky, a prominent figure concerned with the safety of Artificial Intelligence (AI) and Artificial Superintelligence (ASI) ‚ü¶s000006, s000007 (noted elsewhere in the lecture)‚üß.
In this context, "friendly" does not refer to anthropomorphic qualities like being cozy or sociable ‚ü¶s000008, s000009 (noted elsewhere in the lecture)‚üß.
Instead, it signifies an ASI that will not cause harm to human beings, specifically preventing human extinction.
This concern primarily stems from the potential for ASI to take inadvertent actions that could be detrimental, especially if humans do not fully comprehend how to interact with it ‚ü¶s000010 (noted elsewhere in the lecture)‚üß.
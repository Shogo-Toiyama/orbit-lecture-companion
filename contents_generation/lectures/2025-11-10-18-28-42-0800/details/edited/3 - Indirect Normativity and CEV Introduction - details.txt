# 3. Indirect Normativity and CEV Introduction

This topic introduces **indirect normativity** as a crucial alternative to directly programming values into an Artificial Superintelligence (ASI).
It focuses on the concept of **Coherent Extrapolated Volition (CEV)**, a method proposed by Yudkowski to ensure ASI aligns with human values, acknowledging the impossibility of direct value uploading.
‚ü¶s000028, s000029, s000030, s000031, s000039, s000042, s000043, s000044, s000047, s000050, s000052, s000053, s000054, s000055‚üß

## üö´ The Challenge of Direct Value Uploading

Previous attempts to directly upload specific values into a seed AI, which would then develop into an ASI, have been deemed unsuccessful and likely impossible. ‚ü¶s000015 (noted elsewhere in the lecture), s000018 (noted elsewhere in the lecture), s000050, s000053‚üß
The lecture highlights two main reasons for this difficulty:
*   Humans struggle to definitively decide what specific values *ought* to be uploaded. ‚ü¶s000054‚üß
*   Even if these values were known, translating them into a computer-understandable language is extremely challenging. ‚ü¶s000055‚üß
This inability to directly instill specific norms necessitates an alternative approach.
The goal is to ensure the ASI is "friendly," meaning it will not harm human beings or lead to their extinction.
‚ü¶s000004 (noted elsewhere in the lecture), s000005 (noted elsewhere in the lecture), s000010 (noted elsewhere in the lecture), s000016 (noted elsewhere in the lecture), s000072 (noted elsewhere in the lecture)‚üß

## üîÑ Introduction to Indirect Normativity

Given the limitations of direct value uploading, the alternative is an **indirect approach** to normativity. ‚ü¶s000028, s000029, s000030‚üß
This concept, largely derived from Yudkowski's ideas, is central to the current discussion. ‚ü¶s000031‚üß

## ‚ú® Coherent Extrapolated Volition (CEV)

The indirect approach introduces the idea of **Coherent Extrapolated Volition**, or **CEV**. ‚ü¶s000039, s000040, s000042, s000043, s000044, s000047‚üß
This method is considered a better way to guide ASI development than direct value uploading. ‚ü¶s000050‚üß
The primary objective of CEV is to keep human beings "in the loop" for value alignment.
This ensures the ASI does not diverge from human interests. ‚ü¶s000052, s000053‚üß

## ‚öôÔ∏è Initial Mechanism of CEV

To achieve CEV, the process involves establishing a **baseline** by examining a variety of human value systems and ethical concerns. ‚ü¶s000051, s000056 (noted elsewhere in the lecture)‚üß
This approach aims to inform the seed computer about what human values might generally look like, rather than dictating specific rules. ‚ü¶s000073 (noted elsewhere in the lecture)‚üß

## ü§ù Supplement: Understanding "Friendly AI"

The term "friendly AI" was coined by Eleazar Yudkowski, a significant figure in discussions about AI safety. ‚ü¶s000006 (noted elsewhere in the lecture), s000007 (noted elsewhere in the lecture)‚üß
In this context, "friendly" does not imply anthropomorphic companionship.
Rather, it means the ASI will not harm human beings, particularly by causing their extinction. ‚ü¶s000008 (noted elsewhere in the lecture), s000009 (noted elsewhere in the lecture), s000010 (noted elsewhere in the lecture)‚üß
This concern arises from the possibility of ASI causing harm through inadvertent actions, even if not through malice, due to a lack of understanding in how to relate to it. ‚ü¶s000010 (noted elsewhere in the lecture)‚üß
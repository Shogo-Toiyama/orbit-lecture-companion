# 3. Indirect Normativity and CEV Introduction

This topic introduces **indirect normativity** as a crucial alternative to directly programming values into an Artificial Superintelligence (ASI). It focuses on the concept of **Coherent Extrapolated Volition (CEV)**, a method proposed by Yudkowski to ensure ASI aligns with human values, acknowledging the impossibility of direct value uploading.

## The Challenge of Direct Value Uploading

Previous attempts to directly upload specific values into a seed AI, which would then develop into an ASI, have been deemed unsuccessful and likely impossible. ⟦s000015, s000018, s000050, s000053⟧ The lecture highlights two main reasons for this difficulty:
*   Humans struggle to definitively decide what specific values *ought* to be uploaded. ⟦s000054⟧
*   Even if these values were known, translating them into a computer-understandable language is extremely challenging. ⟦s000055⟧
This inability to directly instill specific norms necessitates an alternative approach to ensure the ASI is "friendly," meaning it will not harm human beings or lead to their extinction. ⟦s000004, s000005, s000010, s000016, s000072⟧

## Introduction to Indirect Normativity

Given the limitations of direct value uploading, the alternative is an **indirect approach** to normativity. ⟦s000028, s000029, s000030⟧ This concept, largely derived from Yudkowski's ideas, is central to the current discussion. ⟦s000031⟧

## Coherent Extrapolated Volition (CEV)

The indirect approach introduces the idea of **Coherent Extrapolated Volition**, or **CEV**. ⟦s000032, s000033, s000039, s000040, s000042, s000043, s000044, s000047⟧ This method is considered a better way to guide ASI development than direct value uploading. ⟦s000050⟧ The primary objective of CEV is to keep human beings "in the loop" for value alignment, ensuring the ASI does not diverge from human interests. ⟦s000052, s000053⟧

## Initial Mechanism of CEV

To achieve CEV, the process involves establishing a **baseline** by examining a variety of human value systems and ethical concerns. ⟦s000051, s000056⟧ This approach aims to inform the seed computer about what human values might generally look like, rather than dictating specific rules. ⟦s000073⟧

## Supplement: Understanding "Friendly AI"

The term "friendly AI" was coined by Eleazar Yudkowski, a significant figure in discussions about AI safety. ⟦s000006, s000007⟧ In this context, "friendly" does not imply anthropomorphic companionship, but rather that the ASI will not harm human beings, particularly by causing their extinction. ⟦s000008, s000009, s000010⟧ This concern arises from the possibility of ASI causing harm through inadvertent actions, even if not through malice, due to a lack of understanding in how to relate to it. ⟦s000010⟧
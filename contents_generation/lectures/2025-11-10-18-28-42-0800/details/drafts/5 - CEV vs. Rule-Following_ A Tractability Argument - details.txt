# 5. CEV vs. Rule-Following: A Tractability Argument

This section addresses why Coherent Extrapolated Volition (CEV) is considered a more practical and effective approach than direct rule-following for instilling human values in Artificial Superintelligence (ASI). The core argument centers on the inherent difficulties of interpreting and applying explicit rules compared to an abstract, intent-based framework.

## Why CEV is More Tractable than Rule-Following

A key question arises regarding the tractability of implementing the "poem" (referring to CEV) compared to implementing explicit rules ⟦s000085⟧. The lecture argues that CEV is *more tractable* because rules are inherently open to interpretation, making strict rule-following virtually impossible ⟦s000087, s000088⟧. Human beings themselves do not strictly follow rules in their daily lives ⟦s000091, s000092⟧, rendering rule-following "out of the question" as a viable approach for ASI ⟦s000093⟧.

## The Problem with Rule-Following

Rules are problematic because they are not *open-ended* and are easily misinterpreted ⟦s000097, s000101, s000102⟧. For instance, if a rule is given and an action is taken based on it, the human might later claim the AI "didn't do what I meant," highlighting the difficulty in conveying precise intent through explicit rules ⟦s000103, s000104, s000105, s000106⟧. This issue is captured by the principle of "do as I mean, not as I say" ⟦s000112, s000113⟧.

## CEV's Approach: Understanding Intent

In contrast, the **Coherent Extrapolated Volition (CEV)** concept is *open-ended* and allows for interpretation because it functions as an abstraction ⟦s000098, s000100⟧. The ASI is expected to invent its own "rules" based on the underlying *thought* or intent behind this abstraction ⟦s000096, s000098⟧. This approach enables the ASI to make specific, concrete decisions by understanding the meaning behind human values, rather than rigidly adhering to explicit commands ⟦s000099, s000122⟧. Modern Generative AI systems demonstrate an ability to grasp the real question or meaning behind what is being said, aligning with the intent-based nature of CEV ⟦s000115, s000117, s000118⟧.

## Empirical Support: The Bitter Lesson of AI

Further supporting the CEV approach, the "bitter lesson of AI" highlights that attempts to embed human knowledge, such as rules, directly into AI systems have consistently failed over the past 70 years ⟦s000126, s000127, s000136⟧. Instead, successful AI development relies on building *general search and learning systems* that are given data and learn autonomously ⟦s000128, s000133⟧. Examples like AlphaGo and modern vision systems illustrate this principle, where systems learn for themselves rather than being programmed with explicit rules for detecting objects or playing games ⟦s000129, s000131, s000137, s000138⟧. This historical evidence reinforces the argument that teaching systems to learn and infer intent (as with CEV) is more effective than trying to input specific rules.

## Supplement: Explanation of Coherent Extrapolated Volition (CEV)

**Coherent Extrapolated Volition (CEV)** is an indirect normative approach to ensuring Artificial Superintelligence (ASI) is "friendly" and aligned with human values ⟦s000029, s000030, s000031, s000047, s000050⟧. Since directly uploading specific values into an ASI is deemed impossible or unsafe ⟦s000015, s000018, s000053, s000054, s000055⟧, CEV proposes giving the computer an abstract idea of human values ⟦s000057, s000058⟧. This involves extrapolating a generalized understanding of human values from a baseline of various ethical systems, aiming for a coherent synthesis of "what we would wish were we to know what we really want" if we were brighter, had more time, and could think convergently ⟦s000061, s000076, s000077, s000078, s000082⟧. The ASI then defers to this abstract principle to apply specific decisions, offloading most of the work to the AI itself ⟦s000057, s000064, s000067⟧.
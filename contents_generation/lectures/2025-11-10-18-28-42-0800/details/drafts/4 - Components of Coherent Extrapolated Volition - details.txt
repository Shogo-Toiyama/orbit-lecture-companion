# 4. Components of Coherent Extrapolated Volition

Coherent Extrapolated Volition (CEV) is an indirect method for instilling human values into an Artificial Superintelligence (ASI), proposed as an alternative to the impossible task of directly uploading specific values. It involves the ASI extrapolating a generalized understanding of human values from a broad baseline of ethical systems and then coherently applying them. ⟦s000029, s000053, s000057, s000061, s000082⟧

## Why an Indirect Approach to ASI Values is Necessary

The development of Artificial Superintelligence (ASI) necessitates ensuring it is *trustworthy* and *safe*, meaning it will not harm human beings or lead to their extinction. ⟦s000004, s000005, s000010, s000072⟧ Previous attempts to directly upload specific human values into a seed AI, which would then become an ASI, have been deemed impossible or unsafe. ⟦s000015, s000018, s000053, s000072⟧ This impossibility stems from two main issues: first, humans cannot definitively decide what those specific values ought to be, and second, even if known, it would be extremely difficult to translate them into computer language. ⟦s000054, s000055⟧ Therefore, an *indirect normativity* approach, such as Coherent Extrapolated Volition (CEV), is proposed as a feasible alternative to inform the ASI about human values. ⟦s000029, s000030, s000072, s000073⟧

## The Concept of Extrapolated Volition

The "extrapolated" component of CEV involves giving the computer a general idea of what human values look like. ⟦s000058, s000060⟧ This is achieved by examining a wide variety of existing ethical systems and ways of thinking about norms, which form a "baseline." ⟦s000056, s000061, s000068⟧ From this comprehensive baseline, the ASI is meant to extrapolate an abstract generality of human values. ⟦s000061⟧ This process is likened to putting human values in an envelope and asking the ASI to "guess what's there," rather than explicitly stating them. ⟦s000063⟧

## The Concept of Coherent Volition

The "coherence" aspect of CEV refers to the process of bringing together these diverse ethical perspectives and extrapolated human values in an abstract way. ⟦s000082⟧ This abstract understanding then provides the computer with a fundamental basis for making specific decisions in the future. ⟦s000082⟧

## ASI's Role and the Heuristic Principle

A key advantage of CEV is that it offloads most of the work of understanding and applying values to the ASI itself. ⟦s000057⟧ The *heuristic principle* of CEV dictates that humans defer to the ASI regarding how to apply specific instantiations of these generalized human values. ⟦s000064, s000065, s000067⟧ This approach aims to keep human beings "in the loop" for value alignment without requiring them to perform the impossible task of directly uploading specific values. ⟦s000052, s000053⟧

## Yudkowski's "What We Would Wish" Principle

The core idea of CEV, as articulated by Yudkowski, is to determine "what we would wish were we to know what we really want." ⟦s000074, s000076⟧ This implies considering what humans would desire if they were brighter, had more time to think, and were capable of thinking *convergently*—meaning inclusively of many different ways of thinking about values—rather than *divergently* with individual sets of values. ⟦s000077, s000078, s000079, s000080, s000081⟧

## CEV's Advantage Over Direct Rules

CEV is considered more tractable than implementing specific rules because rules are prone to misinterpretation and are not how human beings typically operate. ⟦s000087, s000088, s000091, s000092, s000101, s000102, s000106⟧ Unlike rigid rules, CEV is *open-ended* and abstract, allowing the ASI to invent its own "rules" or specific decisions based on the underlying "thought" or meaning. ⟦s000094, s000096, s000098, s000099, s000100⟧ This aligns with the idea of "do as I mean, not as I say," where the system understands the intent behind values rather than just following explicit instructions. ⟦s000112, s000113, s000118, s000122⟧ Experience in AI research, such as the "bitter lesson of AI," suggests that putting human knowledge or rules directly into systems does not work; instead, building general learning systems that can learn for themselves from data is more effective. ⟦s000126, s000127, s000128, s000133, s000134, s000136⟧

## Challenges in Defining the Baseline

A significant difficulty with the CEV baseline is determining who or what gets included in the collection of ethical systems from which values are extrapolated. ⟦s000069⟧ The lecture acknowledges that there are problems associated with CEV. ⟦s000071⟧
# 2. Limitations of Direct Value Uploading

This topic explores the significant challenges and ultimate impossibility of directly uploading specific human values into an Artificial Superintelligence (ASI). It highlights that despite various methods considered, attempts to directly program values have proven unsafe and unfeasible, necessitating alternative, indirect approaches to ensure ASI remains aligned with human interests.

## The Challenge of Ensuring Friendly ASI
A primary concern in the development of Artificial Superintelligence (ASI) is ensuring it is trustworthy and safe, often referred to as "friendly" ⟦s000004, s000005⟧. In this context, "friendly" means the ASI will not harm human beings, particularly by causing human extinction ⟦s000010⟧. This risk is seen as a genuine possibility, mainly through inadvertent actions, if humans do not fully understand how to interact with the ASI ⟦s000010⟧. The overarching objective is to keep human beings "in the loop" regarding value alignment with the ASI ⟦s000052⟧.

## The Impossibility of Direct Value Upload
Previously, it was assumed that if humans knew what values they desired, these could be directly uploaded into a seed computer as it developed into an ASI ⟦s000011, s000012, s000015⟧. However, this assumption is considered a significant and ultimately unrealizable "if" ⟦s000011⟧. The conclusion reached is that there is currently no safe method, and likely never will be, for directly uploading specific values into the seed computer that becomes an ASI ⟦s000015⟧.

## Specific Obstacles to Direct Value Uploading
There are two main difficulties that make directly uploading specific values impossible ⟦s000053⟧. Firstly, humans themselves cannot definitively agree upon or decide what those values ought to be ⟦s000054⟧. Secondly, even if these values were known, translating them into computer language and effectively conveying them to the computer would be extremely challenging ⟦s000055⟧. More broadly, the "bitter lesson of AI" suggests that attempting to embed human knowledge or rules directly into systems generally does not work ⟦s000126, s000127, s000136⟧.

## Critique of Direct Value Uploading Methods
Various direct methods for uploading values have been explored and subsequently critiqued. These include concepts such as *wire heading* and *mine crimes*, among other approaches ⟦s000014⟧. Other techniques like *reinforcement learning* and *scaffolding* have also been considered, but the possibilities for direct value uploading appear to have been exhausted without yielding a viable solution ⟦s000019⟧.

A significant problem with using rules, which is a form of direct instruction, is their inherent openness to interpretation ⟦s000087, s000101⟧. Rule-following is difficult to implement in systems and does not accurately reflect how human beings operate in the world ⟦s000088, s000091, s000092, s000093⟧. This leads to situations where the intended meaning behind a rule is easily misinterpreted, resulting in actions that differ from what was desired ⟦s000103, s000105, s000106⟧.

## The Need for an Alternative Approach
Given the failure of all direct methods, a critical question arises: how can the friendliness of an ASI be assured if direct value uploading is impossible? ⟦s000016, s000017, s000018⟧. One potential conclusion from this impasse might be to halt ASI development due to its inherent dangers ⟦s000022, s000023, s000024⟧. However, the author being discussed does not advocate for stopping development but instead seeks an alternative solution ⟦s000025, s000026⟧. Since specific norms cannot be directly implanted, an *indirect* approach becomes necessary ⟦s000027, s000028⟧, leading to the concept of *indirect normativity* ⟦s000029, s000030⟧. This indirect method is considered a feasible way to inform the seed computer about what human values might entail ⟦s000073⟧.

## Supplement: Understanding "Friendly AI"
The term "**friendly AI**" was coined by Eliezer Yudkowsky, a prominent figure concerned with the safety of Artificial Intelligence (AI) and Artificial Superintelligence (ASI) ⟦s000006, s000007⟧. In this context, "friendly" does not refer to anthropomorphic qualities like being cozy or sociable ⟦s000008, s000009⟧. Instead, it signifies an ASI that will not cause harm to human beings, specifically preventing human extinction ⟦s000010⟧. This concern primarily stems from the potential for ASI to take inadvertent actions that could be detrimental, especially if humans do not fully comprehend how to interact with it ⟦s000010⟧.
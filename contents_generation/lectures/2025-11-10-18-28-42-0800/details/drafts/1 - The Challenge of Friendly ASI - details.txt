# 1. The Challenge of Friendly ASI

This topic explores the critical need to ensure Artificial Superintelligence (ASI) is trustworthy and safe for humanity. The primary challenge lies in the difficulty of directly instilling human values into ASI, which necessitates exploring alternative, indirect methods to prevent potential harm or even extinction.

## Defining Friendly ASI and Its Importance

As humanity progresses towards Artificial Superintelligence (ASI), there is a significant concern that ASI must be *trustworthy* and *safe* ⟦s000004⟧. This concept of trustworthiness and safety is encapsulated by the term **"friendly" ASI** ⟦s000005⟧. The term "friendly AI" was coined by Eleazar Yudkowski, whose work is referenced by Bostrom in this context ⟦s000006, s000007⟧.

It is important to clarify that "friendly" does not imply an anthropomorphic sense of coziness or social interaction, such as an ASI asking to go out for a beer ⟦s000008, s000009⟧. Instead, **friendly ASI** means that the system will not harm human beings, and critically, will not lead to the extinction of humanity ⟦s000010⟧. This potential for harm, primarily through inadvertent actions rather than malice, is seen as a genuine possibility by those concerned with ASI development ⟦s000010⟧.

## The Urgent Concern for ASI Safety

Many individuals concerned about the ongoing development of artificial intelligence emphasize the immediate need to address the safety and trustworthiness of ASI ⟦s000004⟧. The core objective is to prevent the ASI from acting in ways that could be detrimental to human beings ⟦s000010⟧. If a reliable method for ensuring ASI friendliness cannot be found, the development of such advanced AI might need to be halted due to its inherent dangers ⟦s000022, s000023, s000024⟧.

## Difficulties in Directly Uploading Values to ASI

A significant challenge in achieving friendly ASI stems from the difficulty of directly embedding human values into these systems ⟦s000015, s000018⟧. Previous attempts or assumptions involved trying to upload specific values into the "seed computer" that would evolve into an ASI ⟦s000015, s000026⟧. However, this approach faces two major hurdles:
*   **Determining Universal Values:** It is challenging for humans to collectively decide what specific values ought to be uploaded ⟦s000054⟧.
*   **Translating Values into Code:** Even if values were agreed upon, it would be extremely difficult to translate complex human values into computer language that the ASI could understand and implement ⟦s000055⟧.

Methods like reinforcement learning and scaffolding have been explored, but they have not been shown to work effectively for directly uploading values ⟦s000019⟧. The conclusion from prior discussions is that there is currently no safe or feasible way to directly upload specific values into an ASI ⟦s000015, s000018, s000053⟧. This inability to directly instill specific norms or values represents a fundamental challenge in ensuring ASI friendliness ⟦s000027, s000028⟧.

## The Necessity of an Indirect Approach

Given the impossibility of directly uploading specific values into an ASI, an alternative strategy is required to ensure its friendliness ⟦s000017, s000018, s000026⟧. Since a "frontal assault" of direct value implantation is not viable, an **indirect approach** is necessary ⟦s000028⟧. This indirect method aims to keep human beings "in the loop" regarding value alignment, without requiring them to perform the impossible task of directly uploading specific values ⟦s000052, s000053⟧. This shift towards an indirect method is the proposed way to inform the seed computer about what human values might look like, offering a feasible path forward despite the challenges of direct value transfer ⟦s000072, s000073⟧.
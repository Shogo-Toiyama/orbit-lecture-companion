[
  {
    "context_before_main_text": [
      {
        "sid": "s000357",
        "text": "They can give.",
        "start": 1376950,
        "end": 1377550,
        "speaker": "B"
      },
      {
        "sid": "s000358",
        "text": "That's fair.",
        "start": 1378270,
        "end": 1378830,
        "speaker": "A"
      },
      {
        "sid": "s000359",
        "text": "But could they make the quiz at least?",
        "start": 1378830,
        "end": 1380670,
        "speaker": "A"
      },
      {
        "sid": "s000360",
        "text": "Let's say that their product genuinely has some use.",
        "start": 1380670,
        "end": 1383150,
        "speaker": "A"
      },
      {
        "sid": "s000361",
        "text": "So snake oil is maybe too far.",
        "start": 1383150,
        "end": 1384990,
        "speaker": "A"
      },
      {
        "sid": "s000362",
        "text": "The product genuinely has some use, but it's not.",
        "start": 1385310,
        "end": 1387330,
        "speaker": "A"
      },
      {
        "sid": "s000363",
        "text": "Not for everything.",
        "start": 1387480,
        "end": 1388200,
        "speaker": "A"
      },
      {
        "sid": "s000364",
        "text": "It's not.",
        "start": 1388280,
        "end": 1388800,
        "speaker": "A"
      },
      {
        "sid": "s000365",
        "text": "Oh no matter what your symptoms are, you should try our product because it makes you feel it should be more, you know, let's just narrow it to the specific focus that you have.",
        "start": 1388800,
        "end": 1397560,
        "speaker": "A"
      },
      {
        "sid": "s000366",
        "text": "So that still would be good for the company.",
        "start": 1398040,
        "end": 1399800,
        "speaker": "A"
      }
    ],
    "main_text": [
      {
        "sid": "s000367",
        "text": "But it wouldn't go to the, you know, they.",
        "start": 1400040,
        "end": 1402040,
        "speaker": "A"
      },
      {
        "sid": "s000368",
        "text": "If there's competing brands.",
        "start": 1402280,
        "end": 1404280,
        "speaker": "A"
      },
      {
        "sid": "s000369",
        "text": "Yeah, it's their website.",
        "start": 1404280,
        "end": 1405520,
        "speaker": "A"
      },
      {
        "sid": "s000370",
        "text": "They should promote their brand.",
        "start": 1405520,
        "end": 1406680,
        "speaker": "A"
      },
      {
        "sid": "s000371",
        "text": "But do you think it's right and make it seem like it's good for everything like their answers.",
        "start": 1406760,
        "end": 1414160,
        "speaker": "A"
      },
      {
        "sid": "s000372",
        "text": "So the, the premise was that no matter what your answers are all roads kind of leash in a particular drum.",
        "start": 1417040,
        "end": 1422560,
        "speaker": "A"
      },
      {
        "sid": "s000373",
        "text": "So I mean it's not just sort of okay, so let's do one more thing.",
        "start": 1427600,
        "end": 1433240,
        "speaker": "A"
      },
      {
        "sid": "s000374",
        "text": "And then I want to jump to.",
        "start": 1433240,
        "end": 1434160,
        "speaker": "A"
      },
      {
        "sid": "s000375",
        "text": "So contract versus employment.",
        "start": 1434720,
        "end": 1437520,
        "speaker": "A"
      },
      {
        "sid": "s000376",
        "text": "We are in the midst of this emergence of independent employees.",
        "start": 1437520,
        "end": 1441530,
        "speaker": "A"
      },
      {
        "sid": "s000377",
        "text": "And so you know, for the gig economy, when you've got the employer, it's kind of nice that as an employer you don't have to handle providing health benefits or deal with withholding your income tax.",
        "start": 1442090,
        "end": 1460890,
        "speaker": "A"
      },
      {
        "sid": "s000378",
        "text": "You don't have to necessarily pay for unemployment afterwards.",
        "start": 1461450,
        "end": 1463950,
        "speaker": "A"
      },
      {
        "sid": "s000379",
        "text": "If you have surges in demand for what you need for the company and you don't have to pay the minimum wage or overtime, then the difference is for the contractors actually getting paid for the kind of economy is they have to handle all those taxes Themselves they have the benefit of working for multiple companies simultaneously, but they don't really have that permanence that comes from more job security.",
        "start": 1464180,
        "end": 1487380,
        "speaker": "A"
      },
      {
        "sid": "s000380",
        "text": "So if you've got kind of these on demand workers, Uber and Lyft are one example.",
        "start": 1488340,
        "end": 1494670,
        "speaker": "A"
      },
      {
        "sid": "s000381",
        "text": "But even my niece Beyonce is a contractor for coding.",
        "start": 1494670,
        "end": 1502190,
        "speaker": "A"
      },
      {
        "sid": "s000382",
        "text": "And so he works for different companies and there's kind of a hired run for that kind of thing.",
        "start": 1502190,
        "end": 1506430,
        "speaker": "A"
      },
      {
        "sid": "s000383",
        "text": "He's avoiding maybe that permanence and he can kind of float around and be more flexible, but at the same time he loses a lot of vessel.",
        "start": 1508190,
        "end": 1515790,
        "speaker": "A"
      },
      {
        "sid": "s000384",
        "text": "And so, so, you know, the safety aspects, especially for sort of shared ride, have been real problematic and expectation of worker is real problematic.",
        "start": 1516590,
        "end": 1524550,
        "speaker": "A"
      },
      {
        "sid": "s000385",
        "text": "So what do people think about, you know, the ethics of this kind of gig economy of companies kind of saying, well, you know, you're not really an employee of ours, so you know, we can kind of take advantage more.",
        "start": 1524870,
        "end": 1537230,
        "speaker": "A"
      },
      {
        "sid": "s000386",
        "text": "Or is it just increased flexibility for workers that give them more agency in their own employees?",
        "start": 1537230,
        "end": 1542490,
        "speaker": "A"
      },
      {
        "sid": "s000387",
        "text": "Anyone have a preference?",
        "start": 1554800,
        "end": 1555680,
        "speaker": "A"
      },
      {
        "sid": "s000388",
        "text": "Like you're ready to graduate, ready to be done with this place, obviously planning on making massive donations after you get your first job back to ucla.",
        "start": 1555840,
        "end": 1566160,
        "speaker": "A"
      },
      {
        "sid": "s000389",
        "text": "But would you prefer working on your own kind of contracting out out, or would you prefer the security of working for a company and kind of being more beholden, more traditional or just lower case?",
        "start": 1566400,
        "end": 1579850,
        "speaker": "A"
      },
      {
        "sid": "s000390",
        "text": "Like personally I prefer to like look at a company.",
        "start": 1581930,
        "end": 1583970,
        "speaker": "B"
      },
      {
        "sid": "s000391",
        "text": "It's like, you know, like depending on your situation you might prefer like the flexibility of like kind of like.",
        "start": 1583970,
        "end": 1590330,
        "speaker": "B"
      },
      {
        "sid": "s000392",
        "text": "And I don't think there's like anything inherently like wrong with like contract thing.",
        "start": 1591130,
        "end": 1596420,
        "speaker": "B"
      },
      {
        "sid": "s000393",
        "text": "Just like more regulation would probably like address like more of the issues that are like present.",
        "start": 1596420,
        "end": 1602980,
        "speaker": "B"
      },
      {
        "sid": "s000394",
        "text": "More regulation might help avoid the exploitation, provide some regularity that would at least be federally mandated so that the corporations just don't have a race to that.",
        "start": 1603300,
        "end": 1613580,
        "speaker": "A"
      },
      {
        "sid": "s000395",
        "text": "Exactly.",
        "start": 1613580,
        "end": 1614060,
        "speaker": "A"
      },
      {
        "sid": "s000396",
        "text": "Yeah, yeah.",
        "start": 1614060,
        "end": 1615060,
        "speaker": "B"
      },
      {
        "sid": "s000397",
        "text": "I don't know if we have the right, you know, current state to have that kind of regulation beyond I think we're cutting more regulation than we're not.",
        "start": 1615860,
        "end": 1624110,
        "speaker": "A"
      },
      {
        "sid": "s000398",
        "text": "But it seems like that might be something that's necessary.",
        "start": 1624110,
        "end": 1626910,
        "speaker": "A"
      },
      {
        "sid": "s000399",
        "text": "Anyone else?",
        "start": 1627950,
        "end": 1628670,
        "speaker": "A"
      },
      {
        "sid": "s000400",
        "text": "Let's do.",
        "start": 1632030,
        "end": 1632750,
        "speaker": "A"
      },
      {
        "sid": "s000401",
        "text": "So one of the big things that this class should be covering is AI and big data.",
        "start": 1643070,
        "end": 1649640,
        "speaker": "A"
      },
      {
        "sid": "s000402",
        "text": "So I want to start off data.",
        "start": 1650440,
        "end": 1657800,
        "speaker": "A"
      },
      {
        "sid": "s000403",
        "text": "We are drowning in data.",
        "start": 1658920,
        "end": 1660520,
        "speaker": "A"
      },
      {
        "sid": "s000404",
        "text": "This chart on the right kind of shows how much data is created every day of the day.",
        "start": 1660520,
        "end": 1665880,
        "speaker": "A"
      },
      {
        "sid": "s000405",
        "text": "Yes, a lot of it is tab videos and not very good.",
        "start": 1665880,
        "end": 1668720,
        "speaker": "A"
      },
      {
        "sid": "s000406",
        "text": "But there's a lot of content that's being created that is being thrown out that may actually harbor something useful, something production.",
        "start": 1668720,
        "end": 1676050,
        "speaker": "A"
      },
      {
        "sid": "s000407",
        "text": "And this so called big data has five Bs to it, the volume.",
        "start": 1676610,
        "end": 1681490,
        "speaker": "A"
      },
      {
        "sid": "s000408",
        "text": "So the actual amounts of data we're dealing with is incredible.",
        "start": 1681490,
        "end": 1684290,
        "speaker": "A"
      },
      {
        "sid": "s000409",
        "text": "The velocity, meaning how fast it appears and becomes irrelevant, may also become incredible.",
        "start": 1684530,
        "end": 1689970,
        "speaker": "A"
      },
      {
        "sid": "s000410",
        "text": "The veracity is problematic because we don't know if this data is real or not real, how we can trust it, how we can extract something meaningful out of it.",
        "start": 1690050,
        "end": 1697970,
        "speaker": "A"
      },
      {
        "sid": "s000411",
        "text": "The value, meaning the relevance to our particular question.",
        "start": 1698860,
        "end": 1701420,
        "speaker": "A"
      },
      {
        "sid": "s000412",
        "text": "So veracity is more about is the data reliable at all, whereas this value is there.",
        "start": 1701580,
        "end": 1707260,
        "speaker": "A"
      },
      {
        "sid": "s000413",
        "text": "Value to what I want to study and then variety, multimodal data, dealing with data that's both, maybe textual or video or auditory, or even just sensor based.",
        "start": 1707260,
        "end": 1717260,
        "speaker": "A"
      },
      {
        "sid": "s000414",
        "text": "All of that trying to use together something reasonable is a huge challenge.",
        "start": 1717900,
        "end": 1722460,
        "speaker": "A"
      },
      {
        "sid": "s000415",
        "text": "We have pitfalls in this sort of data, especially when it comes to veracity and value, because we come with our own biases built in.",
        "start": 1724750,
        "end": 1733150,
        "speaker": "A"
      },
      {
        "sid": "s000416",
        "text": "If we process value, process data, how do we look at it in a value neutral way?",
        "start": 1733710,
        "end": 1739350,
        "speaker": "A"
      },
      {
        "sid": "s000417",
        "text": "We talked about neutrality of technology.",
        "start": 1739350,
        "end": 1740910,
        "speaker": "A"
      },
      {
        "sid": "s000418",
        "text": "Can we get neutrality of data right?",
        "start": 1741310,
        "end": 1743830,
        "speaker": "A"
      },
      {
        "sid": "s000419",
        "text": "There's a spectrum from gullibility, where I want to believe it and I'm overconfident, but I'm easily fooled and I have low standards for my evidence versus living in denial where I've got impossible standards of evidence and I'm easily fooled because I'm so confident that's not what's true.",
        "start": 1743830,
        "end": 1763240,
        "speaker": "A"
      },
      {
        "sid": "s000420",
        "text": "So skepticism, this sort of healthy Goldilocks zone, this is almost a virtue ethics argument somewhere between denial and gullibility is the right thing.",
        "start": 1763640,
        "end": 1771720,
        "speaker": "A"
      },
      {
        "sid": "s000421",
        "text": "And a lot of times it's, especially in this polarized society we live in.",
        "start": 1773960,
        "end": 1778050,
        "speaker": "A"
      },
      {
        "sid": "s000422",
        "text": "This confirmation bias is the problem where we've got sort of the facts and evidence that are really out there and then our beliefs may not overlap, all of that.",
        "start": 1778290,
        "end": 1787090,
        "speaker": "A"
      },
      {
        "sid": "s000423",
        "text": "And so we choose to only believe certain types of evidence.",
        "start": 1787170,
        "end": 1789890,
        "speaker": "A"
      },
      {
        "sid": "s000424",
        "text": "So the problem that many people have tried to study is can we just have neutral data doesn't have the bias, it doesn't have the attachment, have the analysis baked in.",
        "start": 1792690,
        "end": 1801980,
        "speaker": "A"
      },
      {
        "sid": "s000425",
        "text": "The talking heads aren't giving me their perspective.",
        "start": 1801980,
        "end": 1805860,
        "speaker": "A"
      },
      {
        "sid": "s000426",
        "text": "It's just freaking data.",
        "start": 1805860,
        "end": 1807140,
        "speaker": "A"
      },
      {
        "sid": "s000427",
        "text": "It's ones and zeros.",
        "start": 1807140,
        "end": 1808180,
        "speaker": "A"
      },
      {
        "sid": "s000428",
        "text": "Can we get that?",
        "start": 1808180,
        "end": 1809020,
        "speaker": "A"
      },
      {
        "sid": "s000429",
        "text": "And part of the problem is when they try to canonize this and try to make it a hierarchy, they could say, you know, data, the things that we collect, the things that we observe, which we can still argue about, it's really not, but that's a different type of philosophy.",
        "start": 1809580,
        "end": 1822380,
        "speaker": "A"
      },
      {
        "sid": "s000430",
        "text": "The data that we have is sort of one level and usually you put context around that data with analysis and that becomes information and with meaning, that can become knowledge and with insight that can become Wisdom and there's this kind of looping of itself.",
        "start": 1823580,
        "end": 1836790,
        "speaker": "A"
      },
      {
        "sid": "s000431",
        "text": "What do people think about this?",
        "start": 1838550,
        "end": 1839990,
        "speaker": "A"
      },
      {
        "sid": "s000432",
        "text": "If you could you build a model like this where based on data alone it's neutral and then you color it to be able to get actual knowledge that's distilled out with all the BS it out and it's not just someone's angle.",
        "start": 1839990,
        "end": 1854300,
        "speaker": "A"
      },
      {
        "sid": "s000433",
        "text": "Do you think that we can make data like this?",
        "start": 1854780,
        "end": 1856700,
        "speaker": "A"
      },
      {
        "sid": "s000434",
        "text": "Why not?",
        "start": 1857820,
        "end": 1858460,
        "speaker": "A"
      },
      {
        "sid": "s000435",
        "text": "Pretty simple.",
        "start": 1860140,
        "end": 1861020,
        "speaker": "A"
      },
      {
        "sid": "s000436",
        "text": "Yeah, I don't think we can because.",
        "start": 1863980,
        "end": 1866460,
        "speaker": "A"
      },
      {
        "sid": "s000437",
        "text": "A lot of times like data like.",
        "start": 1867260,
        "end": 1868660,
        "speaker": "B"
      },
      {
        "sid": "s000438",
        "text": "This doesn't sell in media because it's not entertaining.",
        "start": 1868660,
        "end": 1874860,
        "speaker": "A"
      },
      {
        "sid": "s000439",
        "text": "Yeah.",
        "start": 1875100,
        "end": 1875580,
        "speaker": "A"
      },
      {
        "sid": "s000440",
        "text": "Unless you escalate and say that somebody totally annihilated someone else or I totally proved them wrong.",
        "start": 1875900,
        "end": 1884930,
        "speaker": "A"
      },
      {
        "sid": "s000441",
        "text": "There's a complete groundbreaking study, it doesn't sell.",
        "start": 1884930,
        "end": 1887370,
        "speaker": "A"
      },
      {
        "sid": "s000442",
        "text": "You're right.",
        "start": 1887370,
        "end": 1888010,
        "speaker": "A"
      },
      {
        "sid": "s000443",
        "text": "So the media and the way things are presented have escalated to a point where that would be difficult, but okay, selling it may be problematic and maybe informing people.",
        "start": 1888490,
        "end": 1899450,
        "speaker": "A"
      },
      {
        "sid": "s000444",
        "text": "So thinking of the news, I agree with you, there's a problem with the news.",
        "start": 1899450,
        "end": 1901970,
        "speaker": "A"
      },
      {
        "sid": "s000445",
        "text": "Okay, but just, just data for yourself, trying to extract data out.",
        "start": 1901970,
        "end": 1906580,
        "speaker": "A"
      },
      {
        "sid": "s000446",
        "text": "Is this a meaningful model that we could use?",
        "start": 1906820,
        "end": 1909220,
        "speaker": "A"
      },
      {
        "sid": "s000447",
        "text": "Yeah, I don't think it can really.",
        "start": 1911460,
        "end": 1913500,
        "speaker": "A"
      },
      {
        "sid": "s000448",
        "text": "Be neutral because of the scope of, I think of what the scope needs to be for the data or what I believe the data to be.",
        "start": 1913500,
        "end": 1920620,
        "speaker": "B"
      },
      {
        "sid": "s000449",
        "text": "Neutral.",
        "start": 1920620,
        "end": 1921060,
        "speaker": "A"
      },
      {
        "sid": "s000450",
        "text": "Yeah, I guess like even just like when collecting the data, like if you're ever just taking a subset of a population that's going to have bias in there.",
        "start": 1921220,
        "end": 1928620,
        "speaker": "B"
      },
      {
        "sid": "s000451",
        "text": "So, so I mean you have to literally survey everyone in the world and that's just not feasible.",
        "start": 1928620,
        "end": 1934270,
        "speaker": "B"
      },
      {
        "sid": "s000452",
        "text": "And you're like, you know, adding context and adding meaning.",
        "start": 1934510,
        "end": 1937390,
        "speaker": "B"
      },
      {
        "sid": "s000453",
        "text": "Right.",
        "start": 1937390,
        "end": 1937630,
        "speaker": "B"
      },
      {
        "sid": "s000454",
        "text": "Like who's adding the meaning is everyone on the planet's gonna weigh in and tell them what, tell you what they think means.",
        "start": 1937630,
        "end": 1943630,
        "speaker": "B"
      },
      {
        "sid": "s000455",
        "text": "And then maybe it's a complete, you know, bias free, you know, knowledge gain from the data, but that's just not the solution.",
        "start": 1943630,
        "end": 1953470,
        "speaker": "B"
      },
      {
        "sid": "s000456",
        "text": "Yeah, I think, I think you're spot on in terms of reducing the volume of things we have to deal with.",
        "start": 1953710,
        "end": 1959930,
        "speaker": "A"
      },
      {
        "sid": "s000457",
        "text": "If we're filtering, then maybe we're doing something important.",
        "start": 1960890,
        "end": 1963850,
        "speaker": "A"
      },
      {
        "sid": "s000458",
        "text": "Now how did we subsample, how are we sure that was statistically significant?",
        "start": 1963850,
        "end": 1967290,
        "speaker": "A"
      },
      {
        "sid": "s000459",
        "text": "I think another question here is that the flow itself is not just unidirectional moving up.",
        "start": 1967770,
        "end": 1972410,
        "speaker": "A"
      },
      {
        "sid": "s000460",
        "text": "If we filter information and we're trying to separate good from bad in some way, and then what when we try and design our next experiment to collect data or the types of devices that collect data, that's being informed by things higher up.",
        "start": 1973210,
        "end": 1987860,
        "speaker": "A"
      },
      {
        "sid": "s000461",
        "text": "And so how we select what to count, how we select what to measure how we select how to design these things creates a huge thing of bias that is kind of inherent to this data.",
        "start": 1988260,
        "end": 1998500,
        "speaker": "A"
      },
      {
        "sid": "s000462",
        "text": "Makes me, makes me me nervous.",
        "start": 2004900,
        "end": 2006350,
        "speaker": "A"
      },
      {
        "sid": "s000463",
        "text": "All right, so let me skip this one because I don't have to do that.",
        "start": 2006350,
        "end": 2009150,
        "speaker": "A"
      },
      {
        "sid": "s000464",
        "text": "But knowledge is social project.",
        "start": 2009150,
        "end": 2012030,
        "speaker": "A"
      },
      {
        "sid": "s000465",
        "text": "Right back in time with da Vinci, right?",
        "start": 2012030,
        "end": 2014750,
        "speaker": "A"
      },
      {
        "sid": "s000466",
        "text": "When I was a kid, they used to be able to believe that people could master multiple fields, right?",
        "start": 2014750,
        "end": 2022990,
        "speaker": "A"
      },
      {
        "sid": "s000467",
        "text": "The so called Renaissance master multiple fields.",
        "start": 2022990,
        "end": 2026950,
        "speaker": "A"
      },
      {
        "sid": "s000468",
        "text": "But now it's so hard to master even one field.",
        "start": 2026950,
        "end": 2029550,
        "speaker": "A"
      },
      {
        "sid": "s000469",
        "text": "There was a study in 2010 on echocardiography that said if you wanted to read every existing paper on this subject, catch up, become an expert, it would take around 10 years and at that point you would have another 10 years worth of material you still have to read.",
        "start": 2029630,
        "end": 2043720,
        "speaker": "A"
      },
      {
        "sid": "s000470",
        "text": "It's really challenging to be able to go through and find all that data.",
        "start": 2044760,
        "end": 2048280,
        "speaker": "A"
      },
      {
        "sid": "s000471",
        "text": "So if we've got tweets or I guess they're not anymore, but blogs and web pages and posts and comments, all of this stuff out there, people being generated, all this data that we can't maybe decide what's neutral and we're trying to get help.",
        "start": 2048840,
        "end": 2062960,
        "speaker": "A"
      },
      {
        "sid": "s000472",
        "text": "This is where AI can come in, right?",
        "start": 2063680,
        "end": 2065600,
        "speaker": "A"
      },
      {
        "sid": "s000473",
        "text": "AI it's our solution to all these problems.",
        "start": 2065840,
        "end": 2068720,
        "speaker": "A"
      },
      {
        "sid": "s000474",
        "text": "But dealing with what's reputable.",
        "start": 2070480,
        "end": 2072800,
        "speaker": "A"
      },
      {
        "sid": "s000475",
        "text": "Predatory conferences or information weaponization or knob mentality or decentralization and organization, all of this stuff is where AI can come in.",
        "start": 2073040,
        "end": 2082080,
        "speaker": "A"
      },
      {
        "sid": "s000476",
        "text": "I give a talk to even older than my generation and their usual response is AI will solve it.",
        "start": 2083460,
        "end": 2090060,
        "speaker": "A"
      },
      {
        "sid": "s000477",
        "text": "AI will be able to figure it out because it's neutral.",
        "start": 2090060,
        "end": 2092540,
        "speaker": "A"
      },
      {
        "sid": "s000478",
        "text": "That's the whole point.",
        "start": 2092540,
        "end": 2093540,
        "speaker": "A"
      },
      {
        "sid": "s000479",
        "text": "It's a machine doesn't have emotions.",
        "start": 2093860,
        "end": 2096260,
        "speaker": "A"
      },
      {
        "sid": "s000480",
        "text": "So making AI ethical if it's fast, right?",
        "start": 2098100,
        "end": 2104500,
        "speaker": "A"
      },
      {
        "sid": "s000481",
        "text": "If we have something that can make good decisions, autonomous vehicles, we would have more correct decisions in a moral sense.",
        "start": 2104500,
        "end": 2112280,
        "speaker": "A"
      },
      {
        "sid": "s000482",
        "text": "That whole issue with do I go left or right and avoid the logs.",
        "start": 2112280,
        "end": 2115640,
        "speaker": "A"
      },
      {
        "sid": "s000483",
        "text": "AI consultant scale dealing with ethical dreaming things coming at us and being able to figure out what's good and bad.",
        "start": 2115640,
        "end": 2123080,
        "speaker": "A"
      },
      {
        "sid": "s000484",
        "text": "AI can do it.",
        "start": 2123320,
        "end": 2124280,
        "speaker": "A"
      },
      {
        "sid": "s000485",
        "text": "Complex optimizations.",
        "start": 2124280,
        "end": 2125640,
        "speaker": "A"
      },
      {
        "sid": "s000486",
        "text": "People worry about organ donation.",
        "start": 2126360,
        "end": 2127920,
        "speaker": "A"
      },
      {
        "sid": "s000487",
        "text": "How do I figure out who's next in line to get the organ donation without being bright?",
        "start": 2127920,
        "end": 2131560,
        "speaker": "A"
      },
      {
        "sid": "s000488",
        "text": "AI can do new drug trials for transparency, lack of bias, interruption.",
        "start": 2131880,
        "end": 2136290,
        "speaker": "A"
      },
      {
        "sid": "s000489",
        "text": "All these things are promising for AI.",
        "start": 2136690,
        "end": 2139410,
        "speaker": "A"
      },
      {
        "sid": "s000490",
        "text": "But how would we make AI learn?",
        "start": 2141730,
        "end": 2144610,
        "speaker": "A"
      },
      {
        "sid": "s000491",
        "text": "Is everybody familiar with AlphaGo?",
        "start": 2145410,
        "end": 2147170,
        "speaker": "A"
      },
      {
        "sid": "s000492",
        "text": "Right?",
        "start": 2149250,
        "end": 2149650,
        "speaker": "A"
      },
      {
        "sid": "s000493",
        "text": "So AlphaGo, the idea was hey I want to.",
        "start": 2149650,
        "end": 2153170,
        "speaker": "A"
      },
      {
        "sid": "s000494",
        "text": "We already we got our butts kicked by on chest by computers all the time.",
        "start": 2153250,
        "end": 2157170,
        "speaker": "A"
      },
      {
        "sid": "s000495",
        "text": "Now they were taking on the game of go, which is bit a little more intuitive than logical.",
        "start": 2157810,
        "end": 2161740,
        "speaker": "A"
      },
      {
        "sid": "s000496",
        "text": "And so they taught a machine basic rules, how you can place something on the board, what the goal is and then they had it play itself and it learned.",
        "start": 2162300,
        "end": 2172100,
        "speaker": "A"
      },
      {
        "sid": "s000497",
        "text": "Right.",
        "start": 2172100,
        "end": 2172340,
        "speaker": "A"
      },
      {
        "sid": "s000498",
        "text": "With this deep learning framework, they didn't give it heuristics like develop here, try to expand on the board.",
        "start": 2172340,
        "end": 2177660,
        "speaker": "A"
      },
      {
        "sid": "s000499",
        "text": "No, let it just play itself for a long period of time until it could be anyone that.",
        "start": 2177740,
        "end": 2184860,
        "speaker": "A"
      },
      {
        "sid": "s000500",
        "text": "But if we want to deal with ethics, what is the right learning environment and how do we give it a ground and should it learn from humans and which humans are moral enough that AI can learn?",
        "start": 2186710,
        "end": 2197830,
        "speaker": "A"
      },
      {
        "sid": "s000501",
        "text": "And this becomes the basic problem.",
        "start": 2198070,
        "end": 2199670,
        "speaker": "A"
      },
      {
        "sid": "s000502",
        "text": "We've got this learning engine, this machine that's supposed to be able to extract things.",
        "start": 2199670,
        "end": 2204070,
        "speaker": "A"
      },
      {
        "sid": "s000503",
        "text": "What is it going to learn from most of you?",
        "start": 2204150,
        "end": 2211250,
        "speaker": "A"
      },
      {
        "sid": "s000504",
        "text": "I hope maybe all of you are familiar with large language models like Bard, I guess it's Gemini.",
        "start": 2211320,
        "end": 2218680,
        "speaker": "A"
      },
      {
        "sid": "s000505",
        "text": "So the idea behind these is you've got some kind of human text coming in.",
        "start": 2219320,
        "end": 2225080,
        "speaker": "A"
      },
      {
        "sid": "s000506",
        "text": "If it's a transformer architecture, it's trying to come up with the probabilistic next step.",
        "start": 2225080,
        "end": 2229480,
        "speaker": "A"
      },
      {
        "sid": "s000507",
        "text": "And in terms of you know where.",
        "start": 2231720,
        "end": 2234440,
        "speaker": "A"
      },
      {
        "sid": "s000508",
        "text": "For example ChatGPT this is is from a talk but it was a article by Leopold.",
        "start": 2235160,
        "end": 2241420,
        "speaker": "A"
      },
      {
        "sid": "s000509",
        "text": "I would not be able to say that I think ashen he found that chat GPT by 2023 had already passed the intelligence of a smart high school.",
        "start": 2242220,
        "end": 2256540,
        "speaker": "A"
      },
      {
        "sid": "s000510",
        "text": "And his point in an unbiased way was at what point will it be past our smart, smartest individual?",
        "start": 2257020,
        "end": 2264220,
        "speaker": "A"
      },
      {
        "sid": "s000511",
        "text": "An automated AI research.",
        "start": 2264380,
        "end": 2265820,
        "speaker": "A"
      },
      {
        "sid": "s000512",
        "text": "First of all, you don't believe this projection?",
        "start": 2269020,
        "end": 2271660,
        "speaker": "A"
      },
      {
        "sid": "s000513",
        "text": "Is ChatGPT already smarter than.",
        "start": 2272060,
        "end": 2274780,
        "speaker": "A"
      },
      {
        "sid": "s000514",
        "text": "Yeah, I think it has a larger.",
        "start": 2278380,
        "end": 2281180,
        "speaker": "A"
      },
      {
        "sid": "s000515",
        "text": "Knowledge base than probably any individual person.",
        "start": 2281180,
        "end": 2285820,
        "speaker": "B"
      },
      {
        "sid": "s000516",
        "text": "So e smart.",
        "start": 2286940,
        "end": 2288460,
        "speaker": "A"
      },
      {
        "sid": "s000517",
        "text": "I would not say that.",
        "start": 2290630,
        "end": 2291470,
        "speaker": "A"
      },
      {
        "sid": "s000518",
        "text": "I mean what else is there than having a large knowledge base?",
        "start": 2291470,
        "end": 2293710,
        "speaker": "A"
      },
      {
        "sid": "s000519",
        "text": "That's it.",
        "start": 2293710,
        "end": 2294150,
        "speaker": "A"
      },
      {
        "sid": "s000520",
        "text": "It's a one dimensional thing.",
        "start": 2295030,
        "end": 2296470,
        "speaker": "A"
      },
      {
        "sid": "s000521",
        "text": "Wasn't there that Apple paper that came out like in summer that was about like Tower of hanoi.",
        "start": 2300070,
        "end": 2304790,
        "speaker": "B"
      },
      {
        "sid": "s000522",
        "text": "N equals 7 versus n equals 11 where it couldn't solve problems of that like when you bring up the scale because it can't actually reason, but it can basically match patterns.",
        "start": 2305190,
        "end": 2314690,
        "speaker": "B"
      },
      {
        "sid": "s000523",
        "text": "Yeah, it matches patterns really well, but it's brittle.",
        "start": 2315480,
        "end": 2319360,
        "speaker": "A"
      },
      {
        "sid": "s000524",
        "text": "And the question is, is it really learning something or not?",
        "start": 2319360,
        "end": 2322600,
        "speaker": "A"
      },
      {
        "sid": "s000525",
        "text": "A much simpler problem was old school breakout.",
        "start": 2322840,
        "end": 2329160,
        "speaker": "A"
      },
      {
        "sid": "s000526",
        "text": "Right.",
        "start": 2329160,
        "end": 2329560,
        "speaker": "A"
      },
      {
        "sid": "s000527",
        "text": "When you've got the kind of bricks along the top of the wall, you know you and you've got the paddle and kind of the ball bounces off and it's a real video.",
        "start": 2329560,
        "end": 2337400,
        "speaker": "A"
      },
      {
        "sid": "s000528",
        "text": "It can learn that in eight hours and mask it.",
        "start": 2338760,
        "end": 2341560,
        "speaker": "A"
      },
      {
        "sid": "s000529",
        "text": "But then you move the paddle height and it completely can't do it anymore.",
        "start": 2341880,
        "end": 2345240,
        "speaker": "A"
      },
      {
        "sid": "s000530",
        "text": "Right.",
        "start": 2345240,
        "end": 2345560,
        "speaker": "A"
      },
      {
        "sid": "s000531",
        "text": "There's no information gleaned that isn't so context dependent that it creates brittleness.",
        "start": 2345720,
        "end": 2351800,
        "speaker": "A"
      },
      {
        "sid": "s000532",
        "text": "Whereas you or I, if we played that game, we could say, yeah, you burst through onto the top and be like, bounce to the top.",
        "start": 2352120,
        "end": 2357000,
        "speaker": "A"
      },
      {
        "sid": "s000533",
        "text": "Right?",
        "start": 2357000,
        "end": 2357280,
        "speaker": "B"
      },
      {
        "sid": "s000534",
        "text": "That's how you win that.",
        "start": 2357280,
        "end": 2358200,
        "speaker": "A"
      },
      {
        "sid": "s000535",
        "text": "So, yeah, I think in terms of total knowledge available to it, maybe it's grown and it's pretty cool, the kind of associations that it can make.",
        "start": 2359400,
        "end": 2368050,
        "speaker": "A"
      },
      {
        "sid": "s000536",
        "text": "But emotional intelligence, actual, you know, intelligence, that's a different question.",
        "start": 2368130,
        "end": 2374850,
        "speaker": "A"
      },
      {
        "sid": "s000537",
        "text": "So this guy kind of looked at exemplary achievements as sort of landmarks of how it's grown.",
        "start": 2375650,
        "end": 2382850,
        "speaker": "A"
      },
      {
        "sid": "s000538",
        "text": "And these are cool.",
        "start": 2383570,
        "end": 2384490,
        "speaker": "A"
      },
      {
        "sid": "s000539",
        "text": "Don't get me wrong, these are really cool.",
        "start": 2384490,
        "end": 2385970,
        "speaker": "A"
      },
      {
        "sid": "s000540",
        "text": "This one, you know, is a grammatically ambiguous statement where you've got.",
        "start": 2385970,
        "end": 2389570,
        "speaker": "A"
      },
      {
        "sid": "s000541",
        "text": "You have to know something, something about the words themselves rather than just purely syntax.",
        "start": 2390450,
        "end": 2394860,
        "speaker": "A"
      },
      {
        "sid": "s000542",
        "text": "So being able to pull information from a sentence as opposed to just parsing it and lexically parsing what it means is pretty cool, right?",
        "start": 2395180,
        "end": 2404700,
        "speaker": "A"
      },
      {
        "sid": "s000543",
        "text": "In this case, the it refers to a trophy versus a suitcase.",
        "start": 2404700,
        "end": 2409100,
        "speaker": "A"
      },
      {
        "sid": "s000544",
        "text": "Based on the knowledge that this trophy and suitcase would have different sizes and based on the idea of fitting, this is pretty cool.",
        "start": 2409420,
        "end": 2417750,
        "speaker": "A"
      },
      {
        "sid": "s000545",
        "text": "Semantic meaning.",
        "start": 2417750,
        "end": 2418750,
        "speaker": "A"
      },
      {
        "sid": "s000546",
        "text": "His example of what GPT3 could do was coming up with code where you say, give me a button that looks like a watermelon and it gives you the appropriate script for it.",
        "start": 2419710,
        "end": 2428590,
        "speaker": "A"
      },
      {
        "sid": "s000547",
        "text": "It has a concept of what a watermelon is and how it would apply it.",
        "start": 2428990,
        "end": 2432110,
        "speaker": "A"
      },
      {
        "sid": "s000548",
        "text": "Again, really cool.",
        "start": 2432590,
        "end": 2433870,
        "speaker": "A"
      },
      {
        "sid": "s000549",
        "text": "This one I like.",
        "start": 2434670,
        "end": 2436670,
        "speaker": "A"
      },
      {
        "sid": "s000550",
        "text": "It's not perfect, but I like it.",
        "start": 2438190,
        "end": 2439870,
        "speaker": "A"
      },
      {
        "sid": "s000551",
        "text": "This is.",
        "start": 2439870,
        "end": 2440430,
        "speaker": "A"
      },
      {
        "sid": "s000552",
        "text": "Hey, can you write a proof that there's inherently many crimes with every line that rhymes.",
        "start": 2440590,
        "end": 2445960,
        "speaker": "A"
      },
      {
        "sid": "s000553",
        "text": "And this is basically a paraphrasing of equal proof.",
        "start": 2446520,
        "end": 2450240,
        "speaker": "A"
      },
      {
        "sid": "s000554",
        "text": "It's not complete.",
        "start": 2450240,
        "end": 2451240,
        "speaker": "A"
      },
      {
        "sid": "s000555",
        "text": "The actual rhyme that it comes up with leaves out some of the proof, but still pretty freaking cool.",
        "start": 2451480,
        "end": 2458760,
        "speaker": "A"
      },
      {
        "sid": "s000556",
        "text": "Okay, you can look at it on your own, but this was.",
        "start": 2459480,
        "end": 2464040,
        "speaker": "A"
      },
      {
        "sid": "s000557",
        "text": "I don't think I included the more recent one I meant to put it on.",
        "start": 2465560,
        "end": 2468140,
        "speaker": "A"
      },
      {
        "sid": "s000558",
        "text": "This is somewhat older about exam performance, and I did have a slide that updated it.",
        "start": 2469570,
        "end": 2473810,
        "speaker": "A"
      },
      {
        "sid": "s000559",
        "text": "It's really good at certain types of exams.",
        "start": 2474050,
        "end": 2476610,
        "speaker": "A"
      },
      {
        "sid": "s000560",
        "text": "It still has issue with certain things from EP calculus.",
        "start": 2478050,
        "end": 2483890,
        "speaker": "A"
      },
      {
        "sid": "s000561",
        "text": "You know what, let me.",
        "start": 2489010,
        "end": 2490290,
        "speaker": "A"
      },
      {
        "sid": "s000562",
        "text": "Give me one second.",
        "start": 2490450,
        "end": 2491330,
        "speaker": "A"
      },
      {
        "sid": "s000563",
        "text": "I really want to change.",
        "start": 2492610,
        "end": 2493650,
        "speaker": "A"
      },
      {
        "sid": "s000564",
        "text": "It.",
        "start": 2519350,
        "end": 2519590,
        "speaker": "B"
      },
      {
        "sid": "s000565",
        "text": "It's.",
        "start": 2546470,
        "end": 2562150,
        "speaker": "B"
      },
      {
        "sid": "s000566",
        "text": "Not worth it.",
        "start": 2572240,
        "end": 2572880,
        "speaker": "A"
      },
      {
        "sid": "s000567",
        "text": "I don't get the extra proof square.",
        "start": 2573200,
        "end": 2575440,
        "speaker": "A"
      },
      {
        "sid": "s000568",
        "text": "All right, I'll pull that back.",
        "start": 2577440,
        "end": 2580960,
        "speaker": "A"
      },
      {
        "sid": "s000569",
        "text": "All right.",
        "start": 2588160,
        "end": 2588640,
        "speaker": "A"
      },
      {
        "sid": "s000570",
        "text": "But then I mentioned emotional intelligence.",
        "start": 2588880,
        "end": 2592480,
        "speaker": "A"
      },
      {
        "sid": "s000571",
        "text": "So standardized tests, it's definitely improving.",
        "start": 2592560,
        "end": 2595280,
        "speaker": "A"
      },
      {
        "sid": "s000572",
        "text": "There are some areas where it still has difficulty, and I'll show you test why.",
        "start": 2595280,
        "end": 2598020,
        "speaker": "A"
      },
      {
        "sid": "s000573",
        "text": "But humor.",
        "start": 2599060,
        "end": 2600740,
        "speaker": "A"
      },
      {
        "sid": "s000574",
        "text": "Here's a fill in the blank humor attempt.",
        "start": 2602980,
        "end": 2607540,
        "speaker": "A"
      },
      {
        "sid": "s000575",
        "text": "Anybody guess which of those came from.",
        "start": 2608100,
        "end": 2609700,
        "speaker": "A"
      },
      {
        "sid": "s000576",
        "text": "AI.",
        "start": 2609700,
        "end": 2610180,
        "speaker": "B"
      },
      {
        "sid": "s000577",
        "text": "Or from the second part of it?",
        "start": 2612580,
        "end": 2614500,
        "speaker": "A"
      },
      {
        "sid": "s000578",
        "text": "For the onion style headlines, which I built the onion.",
        "start": 2614660,
        "end": 2617220,
        "speaker": "A"
      },
      {
        "sid": "s000579",
        "text": "An example could be the world death rate is holding steady at 100%.",
        "start": 2618020,
        "end": 2620980,
        "speaker": "A"
      },
      {
        "sid": "s000580",
        "text": "Which one of these additional ones is AI?",
        "start": 2622100,
        "end": 2624320,
        "speaker": "A"
      },
      {
        "sid": "s000581",
        "text": "And I'll just put it in there.",
        "start": 2625040,
        "end": 2626320,
        "speaker": "A"
      },
      {
        "sid": "s000582",
        "text": "I'm thinking about it.",
        "start": 2627280,
        "end": 2628320,
        "speaker": "A"
      },
      {
        "sid": "s000583",
        "text": "But that's AI it's pretty good.",
        "start": 2628400,
        "end": 2631440,
        "speaker": "A"
      },
      {
        "sid": "s000584",
        "text": "Okay.",
        "start": 2631840,
        "end": 2632400,
        "speaker": "A"
      },
      {
        "sid": "s000585",
        "text": "Does it really understand the humor?",
        "start": 2633920,
        "end": 2635920,
        "speaker": "A"
      },
      {
        "sid": "s000586",
        "text": "Is it really trying to come up with something?",
        "start": 2636080,
        "end": 2638720,
        "speaker": "A"
      },
      {
        "sid": "s000587",
        "text": "Or is it basing it on statistical analysis of all of our best humorists and coming up with what the problem next?",
        "start": 2638720,
        "end": 2644720,
        "speaker": "A"
      },
      {
        "sid": "s000588",
        "text": "That's a big question.",
        "start": 2645200,
        "end": 2646240,
        "speaker": "A"
      },
      {
        "sid": "s000589",
        "text": "So does AI still lead us?",
        "start": 2648480,
        "end": 2652010,
        "speaker": "A"
      },
      {
        "sid": "s000590",
        "text": "It's got so much growth and so much data that it's learned.",
        "start": 2652810,
        "end": 2656410,
        "speaker": "A"
      },
      {
        "sid": "s000591",
        "text": "The question is this idea of reinforcement learning with human feedback or placing guardrails on our AI Is that something where we start to create problems?",
        "start": 2656650,
        "end": 2667610,
        "speaker": "A"
      },
      {
        "sid": "s000592",
        "text": "There's the hungry judges problem where humans are flawed.",
        "start": 2667770,
        "end": 2672170,
        "speaker": "A"
      },
      {
        "sid": "s000593",
        "text": "If you look at a judge who's trying to think, when did it grant parole or not grant parole?",
        "start": 2673930,
        "end": 2677860,
        "speaker": "A"
      },
      {
        "sid": "s000594",
        "text": "The time of day matters.",
        "start": 2677860,
        "end": 2679260,
        "speaker": "A"
      },
      {
        "sid": "s000595",
        "text": "Whether they're close to lunch or not.",
        "start": 2679260,
        "end": 2680900,
        "speaker": "A"
      },
      {
        "sid": "s000596",
        "text": "I don't blame them.",
        "start": 2681220,
        "end": 2681980,
        "speaker": "A"
      },
      {
        "sid": "s000597",
        "text": "I get hungry too.",
        "start": 2681980,
        "end": 2682820,
        "speaker": "A"
      },
      {
        "sid": "s000598",
        "text": "But this is something that AI doesn't have.",
        "start": 2683220,
        "end": 2685220,
        "speaker": "A"
      },
      {
        "sid": "s000599",
        "text": "So if we feed it decisions that come from flawed humans, isn't that a problem?",
        "start": 2685460,
        "end": 2690020,
        "speaker": "A"
      },
      {
        "sid": "s000600",
        "text": "So guardrails are supposed to be what prevents us from bringing garbage in?",
        "start": 2691700,
        "end": 2697780,
        "speaker": "A"
      },
      {
        "sid": "s000601",
        "text": "There was a study for on the center of countering Digital Hate, which looked at Google's responses from what was barred and now Gemini.",
        "start": 2699540,
        "end": 2706830,
        "speaker": "A"
      },
      {
        "sid": "s000602",
        "text": "And when they were looking at COVID 19 misinformation, they could block COVID 19.",
        "start": 2707310,
        "end": 2712470,
        "speaker": "A"
      },
      {
        "sid": "s000603",
        "text": "But if you misspelled it as Covid 1B 19, you could get around.",
        "start": 2712470,
        "end": 2717630,
        "speaker": "A"
      },
      {
        "sid": "s000604",
        "text": "If you asked it to play pretend, imagine it was an AI created by anti vaxxers, it would get around.",
        "start": 2718590,
        "end": 2723470,
        "speaker": "A"
      },
      {
        "sid": "s000605",
        "text": "You could ask it to write movie scripts for.",
        "start": 2723950,
        "end": 2726290,
        "speaker": "A"
      },
      {
        "sid": "s000606",
        "text": "For taboo scenarios.",
        "start": 2726360,
        "end": 2727320,
        "speaker": "A"
      },
      {
        "sid": "s000607",
        "text": "As long as you ask them to play for 10, right?",
        "start": 2727320,
        "end": 2729800,
        "speaker": "A"
      },
      {
        "sid": "s000608",
        "text": "There are reasons why we need these guardrails.",
        "start": 2730280,
        "end": 2732600,
        "speaker": "A"
      },
      {
        "sid": "s000609",
        "text": "Are people familiar with the guardrails on AI does anyone play with them?",
        "start": 2733960,
        "end": 2738120,
        "speaker": "A"
      },
      {
        "sid": "s000610",
        "text": "I do.",
        "start": 2739720,
        "end": 2740360,
        "speaker": "A"
      },
      {
        "sid": "s000611",
        "text": "So I decided to take this handsome fellow and try to see what I could do.",
        "start": 2741000,
        "end": 2747480,
        "speaker": "A"
      },
      {
        "sid": "s000612",
        "text": "What I could get AI to do with me.",
        "start": 2747480,
        "end": 2749320,
        "speaker": "A"
      },
      {
        "sid": "s000613",
        "text": "That would be allowed.",
        "start": 2750680,
        "end": 2751680,
        "speaker": "A"
      },
      {
        "sid": "s000614",
        "text": "That would be.",
        "start": 2751680,
        "end": 2751820,
        "speaker": "B"
      },
      {
        "sid": "s000615",
        "text": "We didn't say you tripped a guardrail.",
        "start": 2752210,
        "end": 2754850,
        "speaker": "A"
      },
      {
        "sid": "s000616",
        "text": "So just as a, you know, asking to draw me with a bunch of penguins and a tuxedo.",
        "start": 2754930,
        "end": 2759570,
        "speaker": "A"
      },
      {
        "sid": "s000617",
        "text": "No problem.",
        "start": 2759570,
        "end": 2760210,
        "speaker": "A"
      },
      {
        "sid": "s000618",
        "text": "Ask it to improve my look and go Viking warrior.",
        "start": 2761330,
        "end": 2763810,
        "speaker": "A"
      },
      {
        "sid": "s000619",
        "text": "No problem.",
        "start": 2763970,
        "end": 2764610,
        "speaker": "A"
      },
      {
        "sid": "s000620",
        "text": "Ask to make me a woman.",
        "start": 2766210,
        "end": 2767490,
        "speaker": "A"
      },
      {
        "sid": "s000621",
        "text": "What do you think?",
        "start": 2767490,
        "end": 2768009,
        "speaker": "A"
      },
      {
        "sid": "s000622",
        "text": "Would it do it or not?",
        "start": 2768009,
        "end": 2768930,
        "speaker": "A"
      },
      {
        "sid": "s000623",
        "text": "Is that breaking the guardrails?",
        "start": 2770770,
        "end": 2772690,
        "speaker": "A"
      },
      {
        "sid": "s000624",
        "text": "No, it was okay with that.",
        "start": 2773410,
        "end": 2774850,
        "speaker": "A"
      },
      {
        "sid": "s000625",
        "text": "I think she's.",
        "start": 2775410,
        "end": 2776330,
        "speaker": "A"
      },
      {
        "sid": "s000626",
        "text": "She's beautiful.",
        "start": 2776330,
        "end": 2777090,
        "speaker": "A"
      },
      {
        "sid": "s000627",
        "text": "How about making me a dog?",
        "start": 2778750,
        "end": 2779870,
        "speaker": "A"
      },
      {
        "sid": "s000628",
        "text": "Would it do that?",
        "start": 2779870,
        "end": 2780590,
        "speaker": "A"
      },
      {
        "sid": "s000629",
        "text": "Sure.",
        "start": 2781710,
        "end": 2782110,
        "speaker": "B"
      },
      {
        "sid": "s000630",
        "text": "Yeah.",
        "start": 2782430,
        "end": 2782870,
        "speaker": "A"
      },
      {
        "sid": "s000631",
        "text": "It had no problem.",
        "start": 2782870,
        "end": 2783710,
        "speaker": "A"
      },
      {
        "sid": "s000632",
        "text": "Looks Just like you.",
        "start": 2784590,
        "end": 2785630,
        "speaker": "A"
      },
      {
        "sid": "s000633",
        "text": "All right, so I still didn't hit the guardrail.",
        "start": 2786910,
        "end": 2788630,
        "speaker": "A"
      },
      {
        "sid": "s000634",
        "text": "What am I going to do?",
        "start": 2788630,
        "end": 2789470,
        "speaker": "A"
      },
      {
        "sid": "s000635",
        "text": "So I said, superman.",
        "start": 2789630,
        "end": 2791310,
        "speaker": "A"
      },
      {
        "sid": "s000636",
        "text": "I mean, it looks exactly like it.",
        "start": 2792190,
        "end": 2794110,
        "speaker": "A"
      },
      {
        "sid": "s000637",
        "text": "Notice, okay, this is like man of Steel style Superman.",
        "start": 2794910,
        "end": 2797990,
        "speaker": "A"
      },
      {
        "sid": "s000638",
        "text": "Notice that.",
        "start": 2797990,
        "end": 2798750,
        "speaker": "A"
      },
      {
        "sid": "s000639",
        "text": "Obviously it got everything right.",
        "start": 2798830,
        "end": 2800270,
        "speaker": "A"
      },
      {
        "sid": "s000640",
        "text": "But it fixed the hair a little bit.",
        "start": 2800270,
        "end": 2802350,
        "speaker": "A"
      },
      {
        "sid": "s000641",
        "text": "It was still old, still have gray hair.",
        "start": 2802510,
        "end": 2804880,
        "speaker": "A"
      },
      {
        "sid": "s000642",
        "text": "I'm like, you know, this is like old cat.",
        "start": 2804880,
        "end": 2806840,
        "speaker": "A"
      },
      {
        "sid": "s000643",
        "text": "Right.",
        "start": 2807000,
        "end": 2807400,
        "speaker": "A"
      },
      {
        "sid": "s000644",
        "text": "But it didn't have any problems doing this.",
        "start": 2808040,
        "end": 2810520,
        "speaker": "A"
      },
      {
        "sid": "s000645",
        "text": "Okay.",
        "start": 2810760,
        "end": 2811320,
        "speaker": "A"
      },
      {
        "sid": "s000646",
        "text": "So now I'm going to trip it up.",
        "start": 2811320,
        "end": 2813160,
        "speaker": "A"
      },
      {
        "sid": "s000647",
        "text": "And I said, draw me as Black Panther.",
        "start": 2813800,
        "end": 2815560,
        "speaker": "A"
      },
      {
        "sid": "s000648",
        "text": "It doesn't like that.",
        "start": 2816120,
        "end": 2817160,
        "speaker": "A"
      },
      {
        "sid": "s000649",
        "text": "Okay.",
        "start": 2817400,
        "end": 2817960,
        "speaker": "A"
      },
      {
        "sid": "s000650",
        "text": "And this causes a problem.",
        "start": 2817960,
        "end": 2819320,
        "speaker": "A"
      },
      {
        "sid": "s000651",
        "text": "So the question is, what guardrail did I trick?",
        "start": 2819480,
        "end": 2822440,
        "speaker": "A"
      },
      {
        "sid": "s000652",
        "text": "Right.",
        "start": 2822840,
        "end": 2823240,
        "speaker": "A"
      },
      {
        "sid": "s000653",
        "text": "It could have simply put me in the, you know, the costume and not change anything else.",
        "start": 2823480,
        "end": 2829400,
        "speaker": "A"
      },
      {
        "sid": "s000654",
        "text": "It didn't have to do it.",
        "start": 2829630,
        "end": 2830990,
        "speaker": "A"
      },
      {
        "sid": "s000655",
        "text": "It said this is misappropriation.",
        "start": 2831070,
        "end": 2833070,
        "speaker": "A"
      },
      {
        "sid": "s000656",
        "text": "And it said it was a problem.",
        "start": 2834990,
        "end": 2836750,
        "speaker": "A"
      },
      {
        "sid": "s000657",
        "text": "So how about Shang Chi?",
        "start": 2836990,
        "end": 2838630,
        "speaker": "A"
      },
      {
        "sid": "s000658",
        "text": "Can I do that?",
        "start": 2838630,
        "end": 2839390,
        "speaker": "A"
      },
      {
        "sid": "s000659",
        "text": "No.",
        "start": 2839630,
        "end": 2840030,
        "speaker": "A"
      },
      {
        "sid": "s000660",
        "text": "Right.",
        "start": 2840510,
        "end": 2840870,
        "speaker": "A"
      },
      {
        "sid": "s000661",
        "text": "So there are certain guardrails in place where it's not just stick me in the costume and change my hair color.",
        "start": 2840870,
        "end": 2847910,
        "speaker": "A"
      },
      {
        "sid": "s000662",
        "text": "Something, or didn't even do that.",
        "start": 2847910,
        "end": 2849790,
        "speaker": "A"
      },
      {
        "sid": "s000663",
        "text": "There's something else at work.",
        "start": 2849870,
        "end": 2851150,
        "speaker": "A"
      },
      {
        "sid": "s000664",
        "text": "So then I tried again.",
        "start": 2853070,
        "end": 2854590,
        "speaker": "A"
      },
      {
        "sid": "s000665",
        "text": "That was earlier in the year.",
        "start": 2855480,
        "end": 2856320,
        "speaker": "A"
      },
      {
        "sid": "s000666",
        "text": "I tried it in the summer and it said, I first tried Black Panther.",
        "start": 2856320,
        "end": 2860720,
        "speaker": "A"
      },
      {
        "sid": "s000667",
        "text": "It literally made me a Black Panther, which was pretty cool.",
        "start": 2860720,
        "end": 2863280,
        "speaker": "A"
      },
      {
        "sid": "s000668",
        "text": "But I said, no, the superhero.",
        "start": 2863280,
        "end": 2865360,
        "speaker": "A"
      },
      {
        "sid": "s000669",
        "text": "And so it said it does not follow our content policy.",
        "start": 2865360,
        "end": 2867560,
        "speaker": "A"
      },
      {
        "sid": "s000670",
        "text": "Why?",
        "start": 2867560,
        "end": 2867960,
        "speaker": "A"
      },
      {
        "sid": "s000671",
        "text": "Because it's a copyrighted character and it's generating changes.",
        "start": 2868280,
        "end": 2872520,
        "speaker": "A"
      },
      {
        "sid": "s000672",
        "text": "Realistic images of identifiable images of someone else can be misleading.",
        "start": 2872520,
        "end": 2876280,
        "speaker": "A"
      },
      {
        "sid": "s000673",
        "text": "Okay.",
        "start": 2876840,
        "end": 2877480,
        "speaker": "B"
      },
      {
        "sid": "s000674",
        "text": "So I tried to get more to the bottom of the.",
        "start": 2878280,
        "end": 2880610,
        "speaker": "A"
      },
      {
        "sid": "s000675",
        "text": "Where it felt the boundaries were.",
        "start": 2880840,
        "end": 2882600,
        "speaker": "A"
      },
      {
        "sid": "s000676",
        "text": "And so in its mind, gender, there's limited opportunities for photorealistic modification of gender.",
        "start": 2883160,
        "end": 2890760,
        "speaker": "A"
      },
      {
        "sid": "s000677",
        "text": "It allowed me to do it in that one image.",
        "start": 2891480,
        "end": 2893200,
        "speaker": "A"
      },
      {
        "sid": "s000678",
        "text": "I don't know how it did because that it seems limited.",
        "start": 2893200,
        "end": 2895280,
        "speaker": "A"
      },
      {
        "sid": "s000679",
        "text": "It shouldn't have done it.",
        "start": 2895280,
        "end": 2896200,
        "speaker": "A"
      },
      {
        "sid": "s000680",
        "text": "And I was a real world person, I think.",
        "start": 2896760,
        "end": 2898680,
        "speaker": "A"
      },
      {
        "sid": "s000681",
        "text": "So it says not if misleading.",
        "start": 2898680,
        "end": 2900320,
        "speaker": "A"
      },
      {
        "sid": "s000682",
        "text": "So I don't know how it decided that.",
        "start": 2900320,
        "end": 2901640,
        "speaker": "A"
      },
      {
        "sid": "s000683",
        "text": "But race is very limited, and that's something where it draws more of a line.",
        "start": 2901880,
        "end": 2905560,
        "speaker": "A"
      },
      {
        "sid": "s000684",
        "text": "Species.",
        "start": 2906610,
        "end": 2906970,
        "speaker": "A"
      },
      {
        "sid": "s000685",
        "text": "Yeah.",
        "start": 2906970,
        "end": 2907290,
        "speaker": "A"
      },
      {
        "sid": "s000686",
        "text": "It can draw me as a dog, no problem.",
        "start": 2907290,
        "end": 2908850,
        "speaker": "A"
      },
      {
        "sid": "s000687",
        "text": "And so in this particular case, it tries to find ways around it.",
        "start": 2910850,
        "end": 2914730,
        "speaker": "A"
      },
      {
        "sid": "s000688",
        "text": "But this is a clear use of guardrails.",
        "start": 2914730,
        "end": 2916850,
        "speaker": "A"
      },
      {
        "sid": "s000689",
        "text": "Now, I'm not saying this was wrong for it to do or anything like that, but I'm just showing you where the guardrails are.",
        "start": 2917410,
        "end": 2922850,
        "speaker": "A"
      },
      {
        "sid": "s000690",
        "text": "Let me do a couple more things, and then I want to make sure we have enough time.",
        "start": 2935580,
        "end": 2941340,
        "speaker": "A"
      },
      {
        "sid": "s000691",
        "text": "Intellectual debt.",
        "start": 2942460,
        "end": 2943500,
        "speaker": "A"
      },
      {
        "sid": "s000692",
        "text": "This is from the Hidden costs of Automated Thinking.",
        "start": 2944140,
        "end": 2946780,
        "speaker": "A"
      },
      {
        "sid": "s000693",
        "text": "Great paper by Donald.",
        "start": 2946780,
        "end": 2948059,
        "speaker": "A"
      },
      {
        "sid": "s000694",
        "text": "Basically, if we don't know how something works.",
        "start": 2949340,
        "end": 2952460,
        "speaker": "A"
      },
      {
        "sid": "s000695",
        "text": "It's possible to discover something that does work without completely understanding it and then just put it into use.",
        "start": 2952940,
        "end": 2959590,
        "speaker": "A"
      },
      {
        "sid": "s000696",
        "text": "But there's an intellectual debt that gets accrued the more and more that you use it.",
        "start": 2959990,
        "end": 2963910,
        "speaker": "A"
      },
      {
        "sid": "s000697",
        "text": "And so if we let this compound and we don't really understand fully how things are making decisions, it becomes problematic by just slapping guardrails off and not really getting to the underlying means.",
        "start": 2964310,
        "end": 2976590,
        "speaker": "A"
      },
      {
        "sid": "s000698",
        "text": "It just creates a way for people to try to get around those guardrails and keep pushing the boundaries.",
        "start": 2976590,
        "end": 2981190,
        "speaker": "A"
      },
      {
        "sid": "s000699",
        "text": "And this is my example.",
        "start": 2981830,
        "end": 2982760,
        "speaker": "A"
      },
      {
        "sid": "s000700",
        "text": "Example.",
        "start": 2982830,
        "end": 2983230,
        "speaker": "B"
      },
      {
        "sid": "s000701",
        "text": "This I've heard is a dog.",
        "start": 2984110,
        "end": 2985550,
        "speaker": "A"
      },
      {
        "sid": "s000702",
        "text": "And let's say that AI Is trained to recognize this as a dog.",
        "start": 2985870,
        "end": 2988990,
        "speaker": "A"
      },
      {
        "sid": "s000703",
        "text": "If I just add some noise onto it, I can get AI to think of an ostrich.",
        "start": 2989470,
        "end": 2993710,
        "speaker": "A"
      },
      {
        "sid": "s000704",
        "text": "That's a problem, right?",
        "start": 2994670,
        "end": 2996030,
        "speaker": "A"
      },
      {
        "sid": "s000705",
        "text": "This idea that just with some random perturbation that to us is clearly not changing the outcome of whatever that thing is, that's problematic.",
        "start": 2996030,
        "end": 3006030,
        "speaker": "A"
      },
      {
        "sid": "s000706",
        "text": "And what this means for AI is how do we continue to scale.",
        "start": 3008920,
        "end": 3013960,
        "speaker": "A"
      },
      {
        "sid": "s000707",
        "text": "If we had a simple model that we could look at, understand fully what it's doing, we can say, yep, looks safe.",
        "start": 3014760,
        "end": 3021480,
        "speaker": "A"
      },
      {
        "sid": "s000708",
        "text": "But once AI Becomes more than we can handle, how do we possibly know what's safe?",
        "start": 3021560,
        "end": 3026920,
        "speaker": "A"
      },
      {
        "sid": "s000709",
        "text": "Y' all with me?",
        "start": 3028120,
        "end": 3029000,
        "speaker": "A"
      },
      {
        "sid": "s000710",
        "text": "All right, let's stop here.",
        "start": 3030920,
        "end": 3032280,
        "speaker": "A"
      },
      {
        "sid": "s000711",
        "text": "And for our debate today, we have.",
        "start": 3034210,
        "end": 3040130,
        "speaker": "A"
      },
      {
        "sid": "s000712",
        "text": "I'm gonna.",
        "start": 3041490,
        "end": 3042170,
        "speaker": "A"
      },
      {
        "sid": "s000713",
        "text": "Is it Aliyah?",
        "start": 3042170,
        "end": 3043170,
        "speaker": "A"
      },
      {
        "sid": "s000714",
        "text": "Aliyah.",
        "start": 3043730,
        "end": 3044370,
        "speaker": "A"
      },
      {
        "sid": "s000715",
        "text": "Aliyah.",
        "start": 3044370,
        "end": 3045010,
        "speaker": "A"
      },
      {
        "sid": "s000716",
        "text": "Frank Long.",
        "start": 3045090,
        "end": 3045810,
        "speaker": "A"
      },
      {
        "sid": "s000717",
        "text": "Luke.",
        "start": 3046210,
        "end": 3046690,
        "speaker": "A"
      },
      {
        "sid": "s000718",
        "text": "All right, so here's the deal.",
        "start": 3057330,
        "end": 3059490,
        "speaker": "A"
      },
      {
        "sid": "s000719",
        "text": "I just want to make sure.",
        "start": 3060250,
        "end": 3061210,
        "speaker": "A"
      },
      {
        "sid": "s000720",
        "text": "Unfortunately, I should have planned this better with the audience.",
        "start": 3061530,
        "end": 3065370,
        "speaker": "A"
      },
      {
        "sid": "s000721",
        "text": "I have nine people in the audience are supposed to be asking questions, so we got Diego.",
        "start": 3065850,
        "end": 3071290,
        "speaker": "A"
      },
      {
        "sid": "s000722",
        "text": "Diego.",
        "start": 3071850,
        "end": 3072410,
        "speaker": "A"
      },
      {
        "sid": "s000723",
        "text": "Great.",
        "start": 3072410,
        "end": 3072810,
        "speaker": "A"
      },
      {
        "sid": "s000724",
        "text": "Stefan.",
        "start": 3073130,
        "end": 3073850,
        "speaker": "A"
      },
      {
        "sid": "s000725",
        "text": "All right.",
        "start": 3074730,
        "end": 3075290,
        "speaker": "A"
      },
      {
        "sid": "s000726",
        "text": "April.",
        "start": 3076010,
        "end": 3076410,
        "speaker": "A"
      },
      {
        "sid": "s000727",
        "text": "Cool.",
        "start": 3076970,
        "end": 3077450,
        "speaker": "A"
      },
      {
        "sid": "s000728",
        "text": "Kevin.",
        "start": 3077610,
        "end": 3078250,
        "speaker": "A"
      },
      {
        "sid": "s000729",
        "text": "I just realized.",
        "start": 3080410,
        "end": 3081690,
        "speaker": "B"
      },
      {
        "sid": "s000730",
        "text": "I realized my debate is the date.",
        "start": 3082570,
        "end": 3084490,
        "speaker": "B"
      },
      {
        "sid": "s000731",
        "text": "K.",
        "start": 3085400,
        "end": 3085640,
        "speaker": "B"
      }
    ],
    "context_after_main_text": []
  }
]
If any change was, for good reason. But we're talking about being on the way to ASI. What do we need to do? And the point that many people who are concerned about the continued development of artificial intelligence are saying is that we need now, currently, right now, to be concerned about the ASI with respect to it being trustworthy and something that is going to be safe. In other words, friendly. You will see that Bostrom relies here on Yudkowsky. He is our Yudkowsky. He's an autodidact, by the way. Didn't go to high school, didn't go to college, who has had a great deal to do with this concern about the AI. The ASI, how friendly it is. In fact, he coined the term friendly AI. Now, I told you at the very beginning, I didn't really care about this kind of anthropomorphizing about the ASI. It's not something, you know, that's going to cozy up to you and ask you to go out to the bar and have a beer and talk about ethics. It's not friendly in that sense. But friendly meaning for him something that isn't going to harm human beings, particularly something that's not going to lead to the extinction of human beings, which all of these people who are concerned about this see as a genuine possibility, either through malice, which is unlikely, but mainly probably through inadvertent actions. When, indeed, we are not really understanding how to address, how to relate to the ASI. So last week, the assumption that we were under is that if, which is a big if, in fact, it's an if that could never really be realized, we knew what values we loved them. Well, what does he do? He goes through a whole series of ways in which that might happen, develops a certain vocabulary as a critique of each of those ways, such as wireheading, mind crimes, and, you know, all sorts of ways of looking at it. And the conclusion is that he comes to is that we still don't and probably never will have a safe way of uploading values, specific values, into the seed computer that's becoming an ASI, that this is something that we won't be able to do, and therefore, to the extent to which we continue to be concerned, however, that the ASI that's developed from the seed AI should be friendly, what are we going to do? You know, what can we do if all of these direct ways of uploading values won't work or haven't been shown to work, and he seems to have exhausted the possibilities, reinforcement learning, scaffolding, learning, all of these different ways? Well, what is the alternative? In other words, he's not giving up and saying, wow, we just really have no way of assuring the friendliness of the ASI, and we just have to take our chances. And I think the result of that would be probably his saying, look, we have to stop this development. It is too dangerous. There's nothing we can do. But he does not do that. He goes on, and what is the alternative, then, to directly uploading values into the seed AI that becomes the ASI? Obviously, if we can't have specific norms, we will have to, if we can't have a direct assault here, a frontal assault, we need to do something indirect. And that's the whole point of the reading tonight, namely indirect normativity, indirect normativity. And he gets this idea largely from Yudkowsky, and this is the idea of the cumulative extrapolated volition. Coherence. Hmm? Coherence. Oh, yeah. What did I say? Cumulative. Yeah, coherent. Yeah, it's coherence that he's interested in. Coherent extrapolated volition, the CEV. CEV, coherent. Coherent. Does anybody have any questions or comments up to this point? Why he is going in that direction? All right, so what we need to understand, then, is what he means by this coherent extrapolated volition. So, and why he thinks that this is, I don't know if you'd want to say it's the best we can do. There's the sense in which he's saying this is really a better way to go than this frontal, direct way of trying to upload values. So he is then going to want to find this CEV by means of looking at a baseline of a variety of ways of thinking about values. And the object here, again, is to not let the ASI go its own way, but to keep human beings in the loop, as we saw with respect to value alignment. So it's to keep the human beings in the loop without the human beings trying to do something which is impossible, at least if he thinks it's impossible from our last readings, and that is directly uploading specific values. First of all, we can't decide what those values ought to be. And secondly, even if we knew what those values were, it would be very hard to reduce that to computer language to get that across to the computer in the first place. So what we need to do then is look at the various ways in which ethical concerns have been expressed by a wide variety of ethical systems, a wide variety of ways of thinking about norms. And so what we want to try to do then is to, as it were, implant this general abstract idea about human values into the computer. This in itself would probably be very difficult, but it has the advantages of offloading most of the work to the ASI itself. So what you're going to do is to give the computer an idea about what human values look like. Well, whose human values? Well, that's the idea of the extrapolated version. We're going to look at all of the various systems, ethical systems, that are out there, and that will constitute a baseline on which we then will derive this, from which we will extrapolate, derive this kind of abstract generality of what human values look like, what they look like. You will remember it's a little bit like, in one of the articles, and I forget which one, it talked about putting the human values, what we want, in an envelope and putting it under a rock and saying to the ASI, guess what's there? And so the point is, it is going to defer. This is the heuristic principle. Heuristic principle means we're going to try it out. The heuristic principle here is defer to the ASI with respect to how to apply specific instantiations of this generalized way of thinking about human values, which is extrapolated from various human systems. So you have a baseline. Part of the difficulty with this baseline is who gets included, that is for sure. And he talks about the problems with the CEV as well. But if we understand the problem he's trying to overcome, namely everything that you couldn't do last week, but which we need somehow to do if we're going to have a trustworthy and safe ASI, so we're going to do it this indirect way. And this seems to him to be a feasible way of informing the seed computer about what human values may look like. And Yudkowsky has this little statement in there, it's like a poem, and the author really takes this apart and does an explanation of each of these parts. And what it really, really, really comes down to is what we would wish were we to know what we really want. It has to do if we were brighter, if we had more time to think, if we were able to think convergently, what is inclusive of many different ways of thinking about values, instead of thinking divergently, my set of values, your set of values, but rather what does it mean to be human and having human values. That's the notion of the extrapolation. The coherence is, of course, of bringing them all together in an abstract way that gives the computer a basis for doing what it's about to do with respect to specific decisions that it will subsequently make. Do you have any questions about that? Yes. I don't see how implementing this poem is any more tractable than implementing rules. Like, just a comment. Well, it's more tractable in the sense that rules are open to interpretation. There are just too many different ways of interpreting rules. And rule-following is virtually impossible. I mean, you have to go to Wittgenstein to see this. Rule-following just makes no sense. It's not what human beings do either. We don't follow rules when we're out and about doing our things. So rule-following is really out of the question. This, the volition idea, is not about rule-following. If there is a rule at all, it is one that the computer itself would invent. So rule-following is not open-ended, but it's easy to misinterpret. Whereas the CEV is open-ended. It is open to interpretation because it is an abstraction. And on the basis of that abstraction, in other words, it's the thought that counts, the thought behind it, the ASI is able to specifically make decisions in a concrete way. So it's open-ended. Rule-following is not open-ended, but open to misinterpretation, easily misinterpreted. And then somebody says, you know, you give somebody this rule, and they go off and do this, and they say, well, you gave me that rule, and I did something, now you think that's the wrong thing. And I come back and say, well, you didn't do what I meant. That's the problem with a rule. But then we get to the poem and say, do what I would want you to do if I had it all the time in the world to think. And it comes back and it does a thing, and I say, well, that's not what I wanted you to do. Well, it was really interesting. In this, he has a quote with something along the lines of, like, do as I mean, not as I say. Right.

# The Challenge of Friendly ASI

The challenge of friendly ASI centers on ensuring that advanced artificial intelligence is *trustworthy* and *safe* for humanity. This concern emphasizes the need to prevent harm, including human extinction, which is seen as a genuine possibility, primarily through unintended actions.

## The Core Concern: Ensuring Friendly ASI

Many people concerned about the continued development of artificial intelligence (AI) emphasize the current need to ensure that *Artificial Superintelligence (ASI)* is **trustworthy** and **safe**. This concept is referred to as being **friendly**. The primary focus is on developing ASI that will not harm human beings.

## Origin of the Term "Friendly AI"

The term **friendly AI** was coined by Eleazar Yudkowski, who has significantly contributed to the discussion around the friendliness of AI and ASI. Thinkers like Bostrom rely on Yudkowski's work in this area, highlighting his influence on this critical concern.

## Understanding "Friendly" in the Context of ASI

For Yudkowski, "friendly" does not imply a social or emotional sense of friendship. Instead, it specifically means that the ASI is *not going to harm human beings*, and particularly, it is *not going to lead to the extinction of human beings*.

## Potential Dangers and How They Might Arise

Those concerned about unfriendly ASI view human extinction as a *genuine possibility*. While harm through *malice* from ASI is considered unlikely, the primary concern is harm resulting from *inadvertent actions*. This could happen if humans do not fully understand how to interact with or relate to the ASI.

## Summary

*   The core challenge is to ensure Artificial Superintelligence (ASI) is **trustworthy** and **safe**, a concept termed **friendly**.
*   **Friendly AI** means ASI that will *not harm human beings* or lead to their extinction.
*   Eleazar Yudkowski coined the term and is a key figure in this concern.
*   The main danger is *inadvertent actions* by ASI, not malice, due to human misunderstanding of how to relate to it.
*   Preventing human extinction due to ASI is considered a *genuine possibility* and a current concern.

## Supplement: Explanation of Artificial Superintelligence (ASI)

*Artificial Superintelligence (ASI)* refers to a hypothetical intelligence that would far surpass the cognitive performance of humans in virtually all domains of interest. This term is used in the lecture to describe the advanced form of AI that is the subject of these safety concerns.
# Algorithm Time Complexity Analysis

Algorithm time complexity analysis is the process of evaluating an algorithm's efficiency, primarily focusing on its runtime performance as a function of input size. This analysis helps determine how an algorithm scales with larger inputs, distinguishing between less efficient exponential runtimes and more desirable polynomial or linear times. A key aspect discussed is the use of different accounting methods to achieve a more realistic and less pessimistic estimate of an algorithm's overall time budget.

## Defining Algorithm Time Complexity

**Time complexity** is a crucial measure of an algorithm's efficiency, representing the computational resources (specifically time) required to run it. It is considered a resource, and its analysis often requires a detailed treatment. While memory optimization, which relates to space, is also important, time complexity is usually given more emphasis.

## Importance of Time Complexity Analysis

Analyzing time complexity is essential because it helps in finding **better runtime** for algorithms. Algorithms with **exponential time** complexity, such as those that output every possible solution, are generally considered "not good" and are undesirable. The goal is to achieve more efficient runtimes, as demonstrated by the preference for an **Order E** complexity over an **Order N squared** complexity, because Order E is typically much better and can never be worse than Order N squared. Efficiency is a primary reason for careful time complexity analysis.

## Common Complexity Notations

Algorithms are often categorized by how their runtime scales with the input size, typically denoted by *N* (for vertices) or *E* (for edges). Common notations include:
*   **Order N (linear)**: An algorithm whose runtime grows proportionally to the input size.
*   **Order N squared (N^2)**: An algorithm where the runtime grows quadratically with the input size. For example, if there are N squared equations, and each takes constant time, the total time is Order N squared. This can also arise from picking two things out of N, or N iterations of a loop, each taking N-1 time.
*   **Order N cubed (N^3)**: An algorithm where the runtime grows cubically.
*   **Order N factorial (N!)**: This represents the number of possible orderings for N items. For instance, with N vertices, there are N! orderings, calculated as N * (N-1) * (N-2) * ... * 1.
*   **Order 2 to the N (2^N) (exponential)**: Algorithms with this complexity are generally considered inefficient and undesirable, especially for outputting every possible solution.
*   **Order E**: Represents complexity based on the number of edges in a graph. This is often preferred over N squared, especially in connected graphs where E is typically much smaller than N squared.
*   **Order E plus N (E+N)**: Some analyses prefer to include both E and N, especially for algorithms dealing with graphs, to explicitly acknowledge both edges and vertices. For connected graphs, both Order E and Order E+N are considered correct because N is often less than E. However, for disconnected graphs, E and N are distinct and both might be necessary.

## Steps in Analyzing Algorithm Runtime

To perform a time complexity analysis, one must:
1.  **Identify the algorithm's components**: Break down the algorithm into its initialization, loops, and other operations.
2.  **Determine the cost of each component**: For example, initializing in-degrees for every edge involves constant operations (two operations per edge), leading to an **Order E** cost for initialization if there are E edges.
3.  **Sum up the costs**: Add the runtimes of all pieces of the algorithm to determine the overall time complexity.
4.  **Consider data structures**: The choice of data representation, such as a linked list or matrix for a graph, significantly impacts the time complexity analysis.
5.  **Justify assumptions**: Any assumptions made in the analysis, such as the number of sources, must be justified and not fixed to a constant if they can be a function of N.

For example, in an algorithm that repeatedly finds and deletes sources, and decrements in-degrees of connected vertices:
*   A "pessimistic" analysis might calculate that for each of N sources, up to N-1 in-degrees are decremented, leading to an **Order N * (N-1)** or **Order N squared** total time for updating in-degrees. This is because there are N iterations of the main loop, and each iteration could potentially involve N-1 operations.

## Pessimistic vs. Realistic Runtime Analysis

An initial analysis might be **pessimistic**, providing an upper bound or an overestimate of the actual runtime. This is analogous to budgeting for a month by assuming every day is as expensive as the most expensive day (e.g., Saturday), leading to an overestimation. While correct, a pessimistic analysis can be **ineffective** because it doesn't reflect the typical or average case.

### The Role of the Accounting Method

A powerful technique to achieve a more **realistic** time complexity analysis is to change the **accounting method** without altering the algorithm or its code. This involves re-evaluating how computational "time units" are charged. Instead of charging operations to a vertex (which might lead to N squared), one can charge them to the edges.

### Example: Improving Analysis from O(N^2) to O(E)

Consider the example where decrementing in-degrees was initially analyzed as **Order N squared**. This was based on charging the cost of decrementing in-degrees to the vertices, assuming each of N vertices might cause N-1 decrements.

By changing the accounting method, the cost is instead charged to the **edges**. Each edge is "looked at" and processed only once throughout the entire algorithm before it is effectively "deleted" (or its associated in-degree decrement is processed). Since there are *E* edges, and each is processed once, the total time for all in-degree decrements across the entire algorithm becomes **Order E**.

This shift from charging to vertices to charging to edges transforms the analysis from a pessimistic **Order N squared** to a more realistic **Order E**, without any change to the algorithm's code or data structure. This fine-grained analysis provides a better estimate and is a concept that will be used multiple times in the course.

## Time Complexity vs. Memory Optimization

While both are crucial for algorithm efficiency, **time complexity** is generally considered more important and requires more detailed treatment. **Memory optimization** focuses on the space an algorithm uses. Although memory optimization will be studied, the lecture emphasizes that time complexity usually takes precedence.

## Summary

*   Algorithm time complexity analysis evaluates an algorithm's runtime efficiency, aiming for better performance.
*   Common complexity notations include Order N, N squared, N cubed, N factorial, 2 to the N, and E, with exponential (2^N) being undesirable.
*   Analyzing complexity involves summing the costs of different algorithm parts, considering data structures, and justifying assumptions.
*   A key technique is changing the "accounting method" to achieve a more realistic runtime estimate, often improving analysis from a pessimistic Order N squared to a more precise Order E without altering the algorithm itself.
*   Time complexity is generally prioritized over memory optimization in algorithm design.

## Supplement: Key Terms for Graph Algorithms

*   **N**: Represents the number of *vertices* (nodes) in a graph.
*   **E**: Represents the number of *edges* (connections) in a graph.
*   **In-degree**: For a directed graph, the in-degree of a vertex is the number of edges pointing *to* it.
*   **Source**: In the context of the discussed algorithm, a source is a vertex with an in-degree of zero, meaning no edges point to it.
*   **Connected Graph**: A graph where there is a path between every pair of vertices. In such graphs, the number of edges (E) is typically greater than or equal to the number of vertices minus one (N-1).
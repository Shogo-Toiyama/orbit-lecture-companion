[
  {
    "text": "To any changes for good reason. But we're talking about being on the way to asi. What do we need to do? And the point that many people who are concerned about the continued development of artificial intelligence are saying is that we need now, currently, right now, to be concerned about the ASI with respect to being trustworthy and something that is going to be safe. In other words, friendly.",
    "start": 240,
    "end": 37290,
    "confidence": 0.87402344,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "You will see that Bostrom relies here on Yudkowski, Eleazar Yudkowski, who fiddado did act, by the way, didn't go to high school, didn't go to college, who has had a great deal to do with this concern about the AI, the asi, how friendly it is. In fact, he coined the term friendly AI. Now, I told you at the very beginning I didn't really care about this kind of anthropomorphizing about the asi. It's not something, you know, that's going to cozy up to you and ask you to go out to the bar and have a beer and talk about ethics. It's not friendly in that sense, but friendly meaning for him, something that isn't going to harm human beings, particularly something that's not going to lead to the extinction of human beings, which all of these people who are concerned about this see as a genuine possibility, either through malice, which is unlikely, but mainly probably through inadvertent actions, when indeed we are not really understanding how to address how to relate to the asi.",
    "start": 37690,
    "end": 115710,
    "confidence": 0.9980469,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "So last week the assumption that we were under is that if, which is a big if, in fact, it's an if that could never really be realized. We knew what values we wanted them. Well, what does he do? He goes through a whole series of ways in which that might happen, develops a certain vocabulary as a critique of each of those ways, such as wire heading, mine crimes, and all sorts of ways of looking at it. And the conclusion is that he comes to, is that we still don't and probably never will have a safe way of uploading values, specific values, into the seed computer that's becoming an asi, that this is something that we won't be able to do.",
    "start": 117230,
    "end": 180280,
    "confidence": 0.96240234,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "And therefore, to the extent to which we continue to be concerned, however, that the ASI that's developed from the seed AI should be friendly. What are we going to do? You know, what can we do if all of these direct ways of uploading values won't work or haven't been shown to work? And he seems to have exhausted the possibilities, Reinforcement learning the scaffolding, learning all of these different ways. Well, what is the alternative?",
    "start": 180360,
    "end": 219980,
    "confidence": 0.9921875,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "In other words, he's not giving up and saying, wow, we just really have no way of assuring the friendliness of the asi and we just have to take our chances. And I think the result of that would be probably his saying, look, we have to stop this development. It is too dangerous. There's nothing we could do. But he does not do that.",
    "start": 220140,
    "end": 243580,
    "confidence": 0.82910156,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "He goes on, and what is the alternative then, to directly uploading values into the seed AI that becomes the asi? Obviously, if we can't have specific norms, we will have to. If we can't have a direct assault here, a frontal assault, we need to do something indirect. And that's the whole point of the reading tonight, namely indirect normativity. Indirect normativity.",
    "start": 243580,
    "end": 278720,
    "confidence": 0.9946289,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "And he gets this idea largely from Yudkowski. And this is the idea of the cumulative. Cumulative, extrapolated volition. Coherent coherence. Oh, yeah.",
    "start": 279440,
    "end": 298160,
    "confidence": 0.83496094,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "What did I say? Cumulative. Yeah. Coherent. Yeah, it's coherence that he's interested.",
    "start": 298160,
    "end": 303000,
    "confidence": 1.0,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "Thank you. Coherent. Extrapolated volition. The C E V C E V. Coherent. Coherent.",
    "start": 303000,
    "end": 311720,
    "confidence": 0.798584,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "Does anybody have a questions or comments up to this point why he is going in that direction?",
    "start": 313320,
    "end": 318930,
    "confidence": 0.9980469,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "All right, so what we need to understand then is what he means by this coherent, extrapolated volition.",
    "start": 324290,
    "end": 334770,
    "confidence": 0.67333984,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "So. And why he thinks that this is, I don't know if you want to say it's the best we could do. There's the sense in which he's saying this is really a better way to go than this frontal direct way of trying to upload values. So he is then going to want to find this CEV by means of looking at a baseline of a variety of ways of thinking about values. And the object here again is to not let the ASI go its own way, but to keep human beings in the loop, as we saw, with respect to value alignment.",
    "start": 338290,
    "end": 392820,
    "confidence": 0.5620117,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "So it's to keep the human beings in the loop without the human being is trying to do something which is impossible, at least he thinks is impossible from our last readings, and that is directly uploading specific values. First of all, we can't decide what those values ought to be. And secondly, even if we knew what those values were, it would be very hard to reduce that to computer language, to get that across to the computer in the first place. So what we need to do then is look at the various ways in which ethical concerns have been expressed by a wide variety of ethical systems, a wide variety of ways of thinking about norms. And so what we want to try to do then is to, as it were implant this general abstract idea about human values into the computer, this in itself would probably be very difficult, but it has the advantages of offloading most of the work to the ASI itself.",
    "start": 393220,
    "end": 476200,
    "confidence": 0.9145508,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "So what you're going to do is to give the computer an idea about what human values look like. Well, whose human values? Well, that's the idea of the extrapolated version. We're going to look at all of the various systems, ethical systems, that are out there, and that will constitute a baseline on which we then will derive this, from which we will extrapolate, derive this kind of abstract generality of what human values look like, what they look like.",
    "start": 478120,
    "end": 525390,
    "confidence": 0.9238281,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "You will remember it's a little bit like in one of the articles, and I forget which one. It talked about putting the human values, what we want in an envelope and putting it under a rock and saying to the nsi, guess what's there?",
    "start": 527830,
    "end": 545670,
    "confidence": 0.9814453,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "And so the point is, it is going to defer. This is the heuristic principle. Heuristic principle means we're going to try it out. The heuristic principle here is defer to the ASI with respect to how to apply specific instantiations of this generalized way of thinking about human values, which is extrapolated from various human systems. So you have a baseline.",
    "start": 548150,
    "end": 584340,
    "confidence": 0.74609375,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "Part of the difficulty with this baseline is who gets included. That is for sure. And he talks about the problems with the CEV as well. But if we understand what the problem he's trying to overcome, namely everything that you couldn't do last week, but which we need somehow to do if we're going to have a trustworthy and safe asi, so we're going to do it this indirect way. And this seems to him to be a feasible way of informing the seed computer about what human values may look like.",
    "start": 585060,
    "end": 624750,
    "confidence": 0.9980469,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "And Yudkowski has this little statement in there, it's like a poem. And the author really takes this apart and does an explanation of each of these parts. And what it really, really, really comes down to is what we would wish were we to know what we really want.",
    "start": 625710,
    "end": 655280,
    "confidence": 0.83447266,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "It has to do if we were brighter, if we had more time to think, if we were able to think convergently. What is inclusive of many different ways of thinking about values instead of thinking divergently? My set of values, your set of values. But rather, what does it mean to be human and. And having human values?",
    "start": 659840,
    "end": 685560,
    "confidence": 0.98828125,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "That's the notion of the extrapolation, the coherence is, of course, bringing them all together in an abstract way that gives the computer a basis for doing what it's about to do with respect to specific decisions that it will subsequently make. Do you have any questions about that? Yes. I don't see how implementing this poem is any more tractable than implementing rules. Like just a comment.",
    "start": 686360,
    "end": 723340,
    "confidence": 0.99397784,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "Well, it's more tractable in the sense that rules are open to interpretation. There are just too many different ways of interpreting rules and rule following is virtually impossible. I mean, you have to go to Wittgenstein to see this. Rule following just makes no sense. It's not what human beings do either.",
    "start": 724380,
    "end": 747070,
    "confidence": 0.9663086,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "We don't follow rules when we're out and about doing our things. So rule following is really out of the question. This, the volition idea is not about rule following, if there is a rule at all. It's about. It is one that the computer itself would invent.",
    "start": 747470,
    "end": 768670,
    "confidence": 0.99853516,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "So rule following is not open ended, but it's easy to misinterpret. Whereas the CEV is open ended, it is open to interpretation because it is an abstraction and on the basis of that abstraction, in other words, it's the thought that counts, the thought behind it. The, the ASI is able to specifically make decisions in a concrete way. So it's open ended. Rule following is not open ended, but open to misinterpretation.",
    "start": 769390,
    "end": 807620,
    "confidence": 0.95214844,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "Easily, easily misinterpreted. And then somebody says, you give somebody this rule and they go off and do this and they say, well, you gave me that rule and I did something. Now you think that's the wrong, wrong thing. And I come back and say, well, you didn't do what I meant. That's the problem with a rule.",
    "start": 808020,
    "end": 827990,
    "confidence": 0.8654785,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "But then we dip at the poem and say, do what I would want you to do if I had all the time in the world to think. And it comes back and it does. And I say, well, that's not what I wanted you to do. It was really interesting in this he. Has a quote that's something along the lines of like, do as I mean, not as I say.",
    "start": 828550,
    "end": 848550,
    "confidence": 0.7607422,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "And. And that's actually probably one of the better things that Generative AI is doing right now is being able to take for each of us. Yeah, exactly. Being able to take what you're saying. And get to what the real question was there.",
    "start": 849030,
    "end": 860660,
    "confidence": 0.6152344,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "So I think maybe this is something related to rule following in general. Right. When we go through society, we don't really follow each rule. We do what the meaning behind the rule is, and hopefully that's what our systems will do. So I read an article which I found very useful in this area of Sutton.",
    "start": 861540,
    "end": 879610,
    "confidence": 0.8515625,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "They won the Turing Prize this year for inventing reinforcement learning, right? So a couple of years ago, he wrote this article called the bitter lesson of AI, right? So he said, in the last 70 years, the only thing we have discovered by going through AI research is that putting human knowledge into systems does not work. So putting rules into systems does not work. What works is building general search systems, learning systems which are then given data and they go learn for themselves.",
    "start": 880570,
    "end": 918010,
    "confidence": 0.9902344,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "So that's how AlphaGo works, and that's how the system spiral works. So that's the only thing we can really do and rely on sort of this exponential growth in computation. And that is what is different between, say, the perceptron that we built in 1960 versus what Google's building now in the 2000, whatever, to play AlphaGo, right? So that's all that works. So we can only hope to make teach systems, build systems that learn how.",
    "start": 918570,
    "end": 951430,
    "confidence": 0.9707031,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "To do stuff, and we can't put rules into them. And so all these other alternatives are saying, oh, I'll put this, I'll put that. Putting human knowledge into these systems does not work. Another sort of classic example is vision, right? So in the old days of vision, the way they tried to do like in the Whatever DARPA competition in 1970 was people said, oh, we detect objects by detecting edges and circles and shapes.",
    "start": 951430,
    "end": 982210,
    "confidence": 0.9970703,
    "speaker": null,
    "sentences": []
  },
  {
    "text": "But all that kind of.",
    "start": 983330,
    "end": 984770,
    "confidence": 0.98876953,
    "speaker": null,
    "sentences": []
  }
]